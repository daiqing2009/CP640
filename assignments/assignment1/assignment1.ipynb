{"cells":[{"cell_type":"markdown","metadata":{"id":"t1FpH_ujo6EQ"},"source":["# CP640 - Machine Learning\n","## Assignment 1 - Linear Regression¶\n","\n","### Learning Objectives\n","After completing this assignment, you should be comfortable:\n","\n","- Simple feature engineering\n","- Using sklearn to build simple and more complex linear models\n","- Building a data pipeline using pandas\n","- Identifying informative variables through EDA\n","- Feature engineering with categorical variables\n","\n","### Marking Breakdown\n","\n","Question | Points\n","--- | ---\n","Question 1a | 1\n","Question 1b | 1\n","Question 1c | 1\n","Question 1d | 1\n","Question 2a | 1\n","Question 2b | 1\n","Question 3a | 1\n","Question 3b | 1\n","Question 3c | 1\n","Question 3d | 1\n","Question 3e | 1\n","Question 3f | 1\n","Question 3g | 1\n","Question 4 | 1\n","Question 5a | 1\n","Question 5b | 1\n","Question 5c | 1\n","Question 5d | 1\n","Question 5e | 1\n","Question 6a | 1\n","Question 6b | 1\n","Question 7 | 1\n","Total | 22\n","\n","One of the following marks below will be added to the **Total** above.\n","\n","### Code Quality\n","\n","| Rank | Points | Description |\n","| :-- | :-- | :-- |\n","| Youngling | 1 | Code is unorganized, variables names are not descriptive, redundant, memory-intensive, computationally-intensive, uncommented, error-prone, difficult to understand. |\n","| Padawan | 2 | Code is organized, variables names are descriptive, satisfactory utilization of memory and computational resources, satisfactory commenting, readable. |\n","| Jedi | 3 | Code is organized, easy to understand, efficient, clean, a pleasure to read. #cleancode |"]},{"cell_type":"markdown","metadata":{"id":"fBNyghg3o6Ea"},"source":["## Setup Notebook"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"L916ENj3o6Ea"},"outputs":[],"source":["# Import 3rd party libraries\n","import os\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Configure Notebook\n","%matplotlib inline\n","plt.style.use('fivethirtyeight')\n","sns.set_context(\"notebook\")\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"markdown","metadata":{"id":"vhi9GCe4o6Ec"},"source":["# Overview\n","The [Ames](http://jse.amstat.org/v19n3/decock.pdf) dataset consists of 2930 records taken from the Ames, Iowa Assessor’s Office describing houses sold in Ames from 2006 to 2010. The data set has 23 nominal, 23 ordinal, 14 discrete, and 20 continuous variables (and 2 additional observation identifiers). 82 features in total. An explanation of each variable can be found in the included `codebook.txt` file.\n","\n","# Import Data\n","Let's import the training datasets."]},{"cell_type":"markdown","metadata":{"id":"pjYG6boMrMH7"},"source":["### Load a Dataset From the Google Drive to Google Colab\n","First you need to mount your google drive to colab"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18931,"status":"ok","timestamp":1694426826455,"user":{"displayName":"Elham Harirpoush","userId":"05871273108717195568"},"user_tz":240},"id":"14f-xDvCpqUG","outputId":"ee38f4c9-c600-46bc-88d5-0786482867d9"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"gdfywSKUrK6l"},"source":["Now you can read_cvs from google drive by navigating to the location where your files are saved.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jamxhwO6o6Ec"},"outputs":[],"source":["ames_data = pd.read_csv('/content/drive/MyDrive/Fall_2023/assignment1/ames_data.csv')"]},{"cell_type":"markdown","metadata":{"id":"V50Q6P5Ro6Ec"},"source":["Now, let's take a look."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":256},"executionInfo":{"elapsed":343,"status":"ok","timestamp":1694426834237,"user":{"displayName":"Elham Harirpoush","userId":"05871273108717195568"},"user_tz":240},"id":"2pnmYISTo6Ed","outputId":"47a44f65-4882-41cd-a398-802fe5d4fd69"},"outputs":[],"source":["ames_data.head()"]},{"cell_type":"markdown","metadata":{"id":"jZVwe7EKo6Ed"},"source":["The next order of business is getting a feel for the variables in our data. The Ames data set contains information that typical homebuyers would want to know. A more detailed description of each variable is included in `codebook.txt`. You should take some time to familiarize yourself with the codebook before moving forward."]},{"cell_type":"markdown","metadata":{"id":"hzFxJty6o6Ed"},"source":["# 1. Exploratory Data Analysis\n","In this section, we will make a series of exploratory visualizations and interpret them.\n","\n","## Sale Price\n","We begin by examining our target variable `SalePrice` using three different plot types."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":590},"executionInfo":{"elapsed":1345,"status":"ok","timestamp":1694426839243,"user":{"displayName":"Elham Harirpoush","userId":"05871273108717195568"},"user_tz":240},"id":"qdtB6gsMo6Ee","outputId":"742a28dd-cf9a-4bb8-c7b8-2b544e6c91ba"},"outputs":[],"source":["fig, axs = plt.subplots(figsize=(12, 8), nrows=2)\n","\n","sns.distplot(\n","    ames_data['SalePrice'].values,\n","    ax=axs[0]\n",")\n","\n","sns.boxplot(\n","    x=ames_data['SalePrice'].values,\n","    width=0.3,\n","    ax=axs[1],\n","    showfliers=False,\n","    boxprops={'facecolor': 'white'}  # Set box fill color to white\n",")\n","\n","sns.stripplot(\n","    x=ames_data['SalePrice'].values,\n","    jitter=0.4,\n","    size=3,\n","    ax=axs[1],\n","    alpha=0.3\n",")\n","\n","# Align axes\n","spacer = np.max(ames_data['SalePrice']) * 0.05\n","xmin = np.min(ames_data['SalePrice']) - spacer\n","xmax = np.max(ames_data['SalePrice']) + spacer\n","axs[0].set_xlim((xmin, xmax))\n","axs[1].set_xlim((xmin, xmax))\n","\n","# Remove some axis text\n","axs[0].xaxis.set_visible(False)\n","axs[0].yaxis.set_visible(False)\n","axs[1].yaxis.set_visible(False)\n","axs[1].set_xlabel('SalePrice', fontsize=20)\n","\n","# Put the two plots together\n","plt.subplots_adjust(hspace=0)\n","\n","# Adjust boxplot fill to be white\n","axs[1].set_facecolor('white')"]},{"cell_type":"markdown","metadata":{"id":"d3vKSbLSo6Ee"},"source":["Now, let's use the Pandas `describe()` method to look at some descriptive statistics of this variable."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":199,"status":"ok","timestamp":1694426843444,"user":{"displayName":"Elham Harirpoush","userId":"05871273108717195568"},"user_tz":240},"id":"eWv29dTJo6Ee","outputId":"42efc3f5-1a75-4287-e619-63e5ec5cc9e0"},"outputs":[],"source":["ames_data['SalePrice'].describe()"]},{"cell_type":"markdown","metadata":{"id":"1Knxz4QMo6Ee"},"source":["## Question 1a\n","To check your understanding of the graph and summary statistics above, answer the following True or False questions:\n","\n","1. The distribution of SalePrice in the training set is left-skewed.\n","2. The mean of SalePrice is greater than the median.\n","3. At least 25% of the houses in the training set sold for more than $200,000.00."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MNhYlZsio6Ee"},"outputs":[],"source":["# Write your code here.\n","q1a_1 = ...\n","q1a_2 = ...\n","q1a_3 = ..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Mu0yNQO35s0"},"outputs":[],"source":["# Sample code for reading images from your Google Drive\n","# import os\n","# os.chdir('/content/drive/My Drive/path/to/image/folder')\n","# img = plt.imread('image.png')\n","# plt.imshow(img)\n"]},{"cell_type":"markdown","metadata":{"id":"JtP-ang4o6Ef"},"source":["## SalePrice vs Gr_Liv_Area\n","Next, we visualize the association between `SalePrice` and `Gr_Liv_Area`. The `codebook.txt` file tells us that `Gr_Liv_Area` measures \"above grade (ground) living area square feet.\"\n","\n","This variable represents the square footage of the house excluding anything underground. Some additional research (into real estate conventions) reveals that this value also excludes the garage space.\n","\n","## Question 1b\n","Create a cross-plot with `SalePrice` on the y-axis and `Gr_Liv_Area` on the x-axis. Use the Seaborn ploting function `sns.jointplot()`. You plot should look something like this.\n","\n","![alt text](https://drive.google.com/uc?id=1N9EDzkSCI0c9soj8J5ydt0YzIXgZEf1K)\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kNdHQhCro6Ef"},"outputs":[],"source":["# Write your code here.\n","..."]},{"cell_type":"markdown","metadata":{"id":"Ocf8wPn-o6Ef"},"source":["There's certainly an association between `SalePrice` and `Gr_Liv_Area`, and perhaps it's linear, but the spread is wider at larger values of both variables. Also, there seems to be at least one suspicious houses above 5000 square feet that look too inexpensive for its size.\n","\n","## Question 1c\n","Find the Parcel Indentification any houses with `Gr_Liv_Area` greater than 5000 sqft. Create a new variable called `potential_outliers` and assign a list of `PID`'s to it."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":261,"status":"ok","timestamp":1693927886831,"user":{"displayName":"Elham Harirpoush","userId":"05871273108717195568"},"user_tz":240},"id":"du_Oyhago6Ef","outputId":"fd0c9c99-f3ea-4613-8109-c41a0e91df59"},"outputs":[],"source":["# Write your code here.\n","potential_outliers = ...\n","\n","# Print answer\n","print(\"Potential outlier PID's: {}\".format(potential_outliers))"]},{"cell_type":"markdown","metadata":{"id":"Lq8VVdomo6Ef"},"source":["We've looked into these two homes in more detail and have determined that they are true outliers in this data set. They were partial sales, priced far below market value. Therefore, we would like to exlude them from our analysis."]},{"cell_type":"markdown","metadata":{"id":"Z4t1g3Wxo6Ef"},"source":["## Question 1d\n","We could simply filter out these outliers using `Pandas` filtering functionality, but when doing machine learning, its always advantageous to create modular, reusable code. For example, we may want to remove outliers from other features and we may want to operationalize our code as a resuable pipeline.\n","\n","Create a function `remove_outliers`, which removes outliers from a data set based off a `lower` and `upper` limit (non-inclusive). For example, `remove_outliers(training_data, 'Gr_Liv_Area', upper=5000)` should return a DataFrame with only observations that satisfy `Gr_Liv_Area` less than 5000."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XFm-h5h0o6Eg"},"outputs":[],"source":["def remove_outliers(data, variable, lower=-np.inf, upper=np.inf):\n","    \"\"\"\n","    Input:\n","      data (data frame): the table to be filtered\n","      variable (string): the column with numerical outliers\n","      lower (numeric): observations with values lower than this will be removed\n","      upper (numeric): observations with values higher than this will be removed\n","\n","    Output:\n","      a winsorized data frame with outliers removed\n","    \"\"\"\n","\n","    # Write your code here.\n","    return ...\n","\n","# Now, apply your function to remove any outliers\n","ames_data = remove_outliers(ames_data, 'Gr_Liv_Area', upper=5000)\n","\n","# View DataFrame\n","ames_data.head()"]},{"cell_type":"markdown","metadata":{"id":"AWNWpIuTo6Eg"},"source":["We started with 2000 rows in `ames_data`. A quick check."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7cr8TChro6Eg"},"outputs":[],"source":["ames_data.shape[0]"]},{"cell_type":"markdown","metadata":{"id":"Bm03Ecivo6Eg"},"source":["Makes sense!"]},{"cell_type":"markdown","metadata":{"id":"9Fg5kTULo6Eg"},"source":["# 2. Feature Engineering\n","In this section we will create a new feature out of existing ones through a simple data transformation.\n","\n","## Bathrooms\n","Let's create a new feature, which described the total number of bathrooms. We will use the following formula:\n","\n","$$ \\text{TotalBathrooms}=(\\text{BsmtFullBath} + \\text{FullBath}) + \\dfrac{1}{2}(\\text{BsmtHalfBath} + \\text{HalfBath})$$\n","\n","## Question 2a\n","Write a function `add_total_bathrooms(data)` that returns a copy of `data` with an additional column called `total_bathrooms` computed by the formula above. You should treat missing values as zeros."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aKpszcCRo6Eg"},"outputs":[],"source":["def add_total_bathrooms(data):\n","    \"\"\"\n","    Input:\n","      data (data frame): a DataFrane containing at least 4 numeric columns\n","            Bsmt_Full_Bath, Full_Bath, Bsmt_Half_Bath, and Half_Bath\n","    \"\"\"\n","\n","    # Make a copy of data\n","    with_bathrooms = data.copy()\n","\n","    # Write your code here.\n","    with_bathrooms['total_bathrooms'] = ...\n","\n","    return with_bathrooms\n","\n","# Now we can add the new feature\n","ames_data_with_bathrooms = add_total_bathrooms(ames_data)\n","\n","# View DataFrame\n","ames_data_with_bathrooms.head()"]},{"cell_type":"markdown","metadata":{"id":"Y0ASjYAgo6Eh"},"source":["Let's check out answer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MJBM6pjDo6Eh"},"outputs":[],"source":["# Change row_id to check multiple rows\n","row_id = 2\n","print('Bsmt_Full_Bath: {}'.format(ames_data_with_bathrooms.loc[row_id , 'Bsmt_Full_Bath']))\n","print('Full_Bath: {}'.format(ames_data_with_bathrooms.loc[row_id , 'Full_Bath']))\n","print('Bsmt_Half_Bath: {}'.format(ames_data_with_bathrooms.loc[row_id , 'Bsmt_Half_Bath']))\n","print('Half_Bath: {}'.format(ames_data_with_bathrooms.loc[row_id , 'Half_Bath']))\n","print('total_bathrooms: {}'.format(ames_data_with_bathrooms.loc[row_id , 'total_bathrooms']))"]},{"cell_type":"markdown","metadata":{"id":"tEBP7FDco6Eh"},"source":["## Question 2b\n","Create a visualization that clearly shows that `total_bathrooms` is associated with `SalePrice`. Your visualization should avoid overplotting."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5GZYkMZ3o6Eh"},"outputs":[],"source":["# Write your code here.\n","..."]},{"cell_type":"markdown","metadata":{"id":"S6V6A5Afo6Eh"},"source":["# 3. Modelling\n","We've reached the point where we can specify a model. But first, we will load a fresh copy of the data, just in case our code above produced any undesired side-effects.\n","\n","Run the cell below to store a fresh copy of the data from ames_train.csv in a dataframe named full_data. We will also store the number of rows in full_data in the variable full_data_len."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSnaheROo6Eh"},"outputs":[],"source":["# Load a fresh copy of the data and get its length\n","full_data = pd.read_csv('/content/drive/MyDrive/Fall_2023/assignment1/ames_data.csv')\n","full_data_len = len(full_data)\n","full_data.head()"]},{"cell_type":"markdown","metadata":{"id":"GkHOyWYQo6Eh"},"source":["## Question 3a\n","Now, let's split the data set into a training set, a validation set, and a test set. We will use the training set to fit our model's parameters, and we will use the validation set to estimate how well our model will perform on unseen data drawn from the same distribution. If we used all the data to fit our model, we would not have a way to estimate model performance on unseen data. The test set is used as a final unseen dataset and we shouldn't touch our test set until our model is finalized.\n","\n","In the cell below, split the data in full_data into three DataFrames named `train`, `val`, and `test`. Let `train` contain 70% of the data, let `val` contain 15% of the data, and let `test` contain 15% of the data.\n","\n","Use the `train_test_split()` function from `sklearn.model_selection` to perform these splits. Use a `random_state=0` as an argument to `train_test_split()`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m4qa0Iqco6Eh"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Write your code here.\n","...\n","\n","# Print results\n","print('Train {}%'.format(train.shape[0] / full_data.shape[0] * 100))\n","print('Val {}%'.format(val.shape[0] / full_data.shape[0] * 100))\n","print('Test {}%'.format(test.shape[0] / full_data.shape[0] * 100))"]},{"cell_type":"markdown","metadata":{"id":"KoycUsfRo6Eh"},"source":["Lock the test set away and do not predict on it until you've selected your final model."]},{"cell_type":"markdown","metadata":{"id":"yqXsDjUYo6Eh"},"source":["## Reusable Pipeline\n","Throughout this assignment, you should notice that your data flows through a single processing pipeline several times. From a software engineering perspective, it's best to define functions/methods that can apply the pipeline to any dataset. We will now encapsulate our entire pipeline into a single function `process_data`. We select a handful of features to use from the many that are available."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZeNrr2XBo6Ei"},"outputs":[],"source":["def select_columns(data, *columns):\n","    \"\"\"Select only columns passed as arguments.\"\"\"\n","    return data.loc[:, columns]\n","\n","def process_data(data):\n","    \"\"\"Process the data for a guided model.\"\"\"\n","    data = remove_outliers(data, 'Gr_Liv_Area', upper=5000)\n","\n","    # Transform Data, Select Features\n","    data = add_total_bathrooms(data)\n","    data = select_columns(data,\n","                          'SalePrice',\n","                          'Gr_Liv_Area',\n","                          'Garage_Area',\n","                          'total_bathrooms')\n","\n","    # Return predictors and response variables separately\n","    X = data.drop(['SalePrice'], axis = 1)\n","    y = data.loc[:, 'SalePrice']\n","\n","    return X, y"]},{"cell_type":"markdown","metadata":{"id":"670ceC4Go6Ei"},"source":["Now, we can use `process_data` to clean our data, select features, and add our `TotalBathrooms` feature all in one step. This function also splits our data into `X`, a matrix of features, and `y`, a vector of sale prices (our training target).\n","\n","Run the cell below to feed our training and validation data through the pipeline, generating `X_train`, `y_train`, `X_val`, and `y_val`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7OFAt-LIo6Ei"},"outputs":[],"source":["X_train, y_train = process_data(train)\n","X_val, y_val = process_data(val)"]},{"cell_type":"markdown","metadata":{"id":"Dcftuptpo6Eo"},"source":["## Fitting Our First Model\n","We are finally going to fit a model.  The model we will fit can be written as follows:\n","\n","$$\\text{SalePrice} = \\theta_0 + \\theta_1 \\cdot \\text{Gr}\\_\\text{Liv}\\_\\text{Area} + \\theta_2 \\cdot \\text{Garage}\\_\\text{Area} + \\theta_3 \\cdot \\text{total_bathrooms}$$\n","\n","**Note:** Notice that all of our variables are continuous, except for `total_bathrooms`, which takes on discrete ordered values (0, 0.5, 1, 1.5, ...). We'll treat `total_bathrooms` as a continuous quantitative variable in our model for now, but this might not be the best choice. The latter half of this assignment may revisit the issue."]},{"cell_type":"markdown","metadata":{"id":"ghjBeX7Mo6Eo"},"source":["## Question 3b\n","We will use a [`sklearn.linear_model.LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) object as our linear model. In the cell below, create a `LinearRegression` object and name it `linear_model`.\n","\n","**Hint:** See the `fit_intercept` parameter and make sure it is set appropriately. The intercept of our model corresponds to $\\theta_0$ in the equation above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dDz0yfNlo6Eo"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","\n","# Write your code here.\n","linear_model = ..."]},{"cell_type":"markdown","metadata":{"id":"aD6mUnFHo6Ep"},"source":["## Question 3c\n","Now, remove the commenting and fill in the ellipses `...` below with `X_train`, `y_train`, `X_val,` or `y_val`.\n","\n","With the ellipses filled in correctly, the code below should fit our linear model to the training data and generate the predicted sale prices for both the training and validation datasets.\n","\n","Assign your predictions for the training set to `y_fitted` and your predictions to the validation set to `y_predicted`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ngPKD-io6Ep"},"outputs":[],"source":["# Uncomment the lines below and fill in\n","# the ... with X_train, y_train, X_test, or y_test.\n","linear_model.fit(...)\n","y_fitted = linear_model.predict(...)\n","y_predicted = linear_model.predict(...)"]},{"cell_type":"markdown","metadata":{"id":"rc9Cvuqmo6Ep"},"source":["## Question 3d\n","Is our linear model any good at predicting house prices? Let's measure the quality of our model by calculating the Root-Mean-Square Error (RMSE) between our predicted house prices and the true prices stored in `SalePrice`.\n","\n","$$\\text{RMSE} = \\sqrt{\\dfrac{\\sum_{\\text{houses in dataset}}(\\text{actual price of house} - \\text{predicted price of house})^2}{\\text{number of houses in dataset}}}$$\n","\n","In the cell below, write a function named `rmse` that calculates the RMSE of a model.\n","\n","**Hint:** Make sure to vectorize your code. This question can be answered without any `for` statements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3XIbgm_Lo6Ep"},"outputs":[],"source":["def rmse(actual, predicted):\n","    \"\"\"\n","    Calculates RMSE from actual and predicted values\n","    Input:\n","      actual (1D array): vector of actual values\n","      predicted (1D array): vector of predicted/fitted values\n","    Output:\n","      a float, the root-mean square error\n","    \"\"\"\n","\n","    # Write your code here.\n","    return ..."]},{"cell_type":"markdown","metadata":{"id":"HUuUmeoho6Ep"},"source":["## Question 3e\n","Now use your `rmse` function to calculate the training error and validation error in the cell below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QpMCmYaPo6Ep"},"outputs":[],"source":["# Write your code here.\n","training_error = rmse(...)\n","val_error = rmse(...)\n","\n","# Print answers\n","print('Training RMSE: ${}'.format(training_error))\n","print('Validation RMSE: ${}'.format(val_error))"]},{"cell_type":"markdown","metadata":{"id":"wAvKXVPfo6Eq"},"source":["## Question 3f\n","How much does including `total_bathrooms` as a predictor reduce the RMSE of the model on the validation set? That is, what's the difference between the RSME of a model that only includes `Gr_Liv_Area` and `Garage_Area` versus one that includes all three predictors (`Gr_Liv_Area`, `Garage_Area`, and `total_bathrooms`)?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMBa_ZRko6Eq"},"outputs":[],"source":["# Drop 'total_bathrooms'\n","X_train_no_bath = X_train.drop('total_bathrooms', axis=1)\n","X_val_no_bath = X_val.drop('total_bathrooms', axis=1)\n","\n","# Initialize model\n","linear_model_no_bath = LinearRegression(fit_intercept=True)\n","\n","# Fit model\n","linear_model_no_bath.fit(...)\n","y_predicted_no_bath = linear_model_no_bath.predict(...)\n","\n","# Compute training and validation errors\n","val_error_no_bath = rmse(...)\n","\n","# compute error difference\n","val_error_difference = ...\n","\n","# Print results\n","print('Validation RMSE: ${}'.format(val_error))\n","print('Validation RMSE (No Bath): ${}'.format(val_error_no_bath))\n","print('Validation Error Difference: {}'.format(val_error_difference))"]},{"cell_type":"markdown","metadata":{"id":"AFMdd11qo6Eq"},"source":["## Residual Plots\n","One way of understanding the performance (and appropriateness) of a model is through a residual plot. Run the cell below to plot the actual sale prices against the residuals of the model for the validation data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"evcy7vTFo6Eq"},"outputs":[],"source":["residuals = y_val - y_predicted\n","ax = sns.regplot(y_val, residuals)\n","ax.set_xlabel('Sale Price (Validation Data)')\n","ax.set_ylabel('Residuals (Actual Price - Predicted Price)')\n","ax.set_title(\"Residuals vs. Sale Price on Validation Data\");"]},{"cell_type":"markdown","metadata":{"id":"tH7PnZR_o6Eq"},"source":["Ideally, we would see a horizontal line of points at 0 (a perfect prediction!). The next best thing would be a homogenous set of points centered at 0.\n","\n","But alas, our simple model is probably too simple. The most expensive homes are systematically more expensive than our prediction."]},{"cell_type":"markdown","metadata":{"id":"d6djRm-Ao6Eq"},"source":["## Question 3g\n","What changes could you make to your linear model to improve its accuracy and lower the validation error? Suggest at least two things you could try in the cell below, and carefully explain how each change could potentially improve your model's accuracy."]},{"cell_type":"markdown","metadata":{"id":"levGxtZao6Eq"},"source":["*Type your answer here, replacing this text.*"]},{"cell_type":"markdown","metadata":{"id":"XBXPoMXPo6Er"},"source":["# 4. Cross Validation\n","Moving forward, we will now use cross validation to help validate our model instead of explicitly splitting the data into a training and validation set. To do this, we'll need to create a cross-validation function for our `rmse` score.\n","\n","First, let's split the `full_data` again but this time into only two datasets `train` and `test`. Let `train` contain 70% of the data, let `test` contain 30% of the data.\n","\n","Again, we will use the `train_test_split()` function from `sklearn.model_selection` to perform these splits and use a `random_state=0` as an argument to `train_test_split()`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mj67vsqvo6Er"},"outputs":[],"source":["# Split dataset\n","train, test = train_test_split(full_data, test_size=0.30, random_state=0)\n","\n","# Print results\n","print('Train {}%'.format(train.shape[0] / full_data.shape[0] * 100))\n","print('Test {}%'.format(test.shape[0] / full_data.shape[0] * 100))"]},{"cell_type":"markdown","metadata":{"id":"rjcoeCl3o6Er"},"source":["Next, let's use out `process_data` function to get our features (`X_train`) and target ('y_train') for the training dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AL22rY8yo6Er"},"outputs":[],"source":["X_train, y_train = process_data(train)"]},{"cell_type":"markdown","metadata":{"id":"8lJCgUyoo6Er"},"source":["# Question 4\n","Create a cross-validation function, which returns the average `rmse` score from all 5 splits.\n","\n","Hint: `train_index` and `val_index` contain the train and val row indices for `X` and `y`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-OgfBWfo6Er"},"outputs":[],"source":["from sklearn.model_selection import KFold\n","from sklearn.base import clone\n","\n","def cross_validate_rmse(model, X, y):\n","\n","    # Setup\n","    model = clone(model)\n","    five_fold = KFold(n_splits=...)\n","    rmse_values = []\n","\n","    # Iterature thought cv-folds\n","    for train_index, val_index in five_fold.split(X):\n","\n","        # Write your code here.\n","\n","        # Fit model\n","        model.fit(...)\n","\n","        # Append RMSE scores\n","        rmse_values.append(...z)\n","\n","    return rmse_values"]},{"cell_type":"markdown","metadata":{"id":"q1M4htI4o6Er"},"source":["Now, let's apply out new cross-validation function to out training dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RH-Oj77mo6Es"},"outputs":[],"source":["cv_scores = cross_validate_rmse(model=LinearRegression(fit_intercept=True), X=X_train, y=y_train)\n","\n","# Print cv scores\n","print('Cross-validation RMSE scores: {}'.format(cv_scores))\n","print('Cross-validation RMSE scores mean: ${}'.format(np.mean(cv_scores)))\n","print('Cross-validation RMSE scores std: ${}'.format(np.std(cv_scores)))"]},{"cell_type":"markdown","metadata":{"id":"7NXaQhp6o6Es"},"source":["Next, let's compare this to our old validation score from earlier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T07Vv6o1o6Es"},"outputs":[],"source":["print('Validation RMSE score: ${}'.format(val_error))\n","print('Cross-validation RMSE scores mean: ${}'.format(np.mean(cv_scores)))"]},{"cell_type":"markdown","metadata":{"id":"xCiPGqzFo6Es"},"source":["This example clearly demonstrates the function of cross-validation instead of explicitly splitting the data into a training and validation set.\n","\n","We can see that the first cross-validation score for the first split is `53256.63312984414`, which is similar to our original validation score. However, the other 4 cross-validation scores are lower (`40632.040286195836, 40982.685175624334, 43732.46059794499, 42570.16643666945`).\n","\n","Especially when datasets are small, sampling bias can cause differences in performance depending on how a dataset is split. Cross-validation ensures you're able to monitor this variability and evaluate your models properly."]},{"cell_type":"markdown","metadata":{"id":"tFRvPBY9o6Es"},"source":["# 5. More Feature Selection and Engineering\n","The linear model that you created failed to produce accurate estimates of the observed housing prices because the model was too simple. The goal of the next few sections is to guide you through the iterative process of specifying, fitting, and analyzing the performance of more complex linear models used to predict prices of houses in Ames, Iowa.\n","\n","In this section, we identify two more features of the dataset that will increase our linear regression model's accuracy. Additionally, we will implement one-hot encoding so that we can include binary and categorical variables in our improved model.\n","\n","We've used a slightly modified data cleaning pipeline from the first half of the assignment to prepare the data. This data is stored in `ames_data_cleaned.csv`. It consists of 1998 observations and 83 features (we added the feature `total_bathrooms` from the first half of the assignment).\n","\n","First, let's import `ames_data_cleaned.csv`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ma2y5Droo6Es"},"outputs":[],"source":["ames_data_cleaned = pd.read_csv('/content/drive/MyDrive/Fall_2023/assignment1/ames_data_cleaned.csv')\n","ames_data_cleaned.head()"]},{"cell_type":"markdown","metadata":{"id":"Lwq_vpJAo6Es"},"source":["Next, let's split the `ames_data_cleaned` into only two datasets `train` and `test`. Let `train` contain 70% of the data, let `test` contain 30% of the data.\n","\n","Again, we will use the `train_test_split()` function from `sklearn.model_selection` to perform these splits and use a `random_state=0` as an argument to `train_test_split()`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nb8Qy1swo6Et"},"outputs":[],"source":["# Split dataset\n","train_cleaned, test_cleaned = train_test_split(ames_data_cleaned, test_size=0.30, random_state=0)\n","\n","# Print results\n","print('Train {}%'.format(train_cleaned.shape[0] / ames_data_cleaned.shape[0] * 100))\n","print('Test {}%'.format(test_cleaned.shape[0] / ames_data_cleaned.shape[0] * 100))"]},{"cell_type":"markdown","metadata":{"id":"jAcuDMJro6Et"},"source":["## Neighborhood vs Sale Price\n","First, let's take a look at the relationship between neighborhood and sale prices of the houses in our data set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sJRxcrXBo6Et"},"outputs":[],"source":["fig, axs = plt.subplots(nrows=2, figsize=(12, 8))\n","\n","sns.boxplot(\n","    x='Neighborhood',\n","    y='SalePrice',\n","    data=train_cleaned.sort_values('Neighborhood'),\n","    ax=axs[0]\n",")\n","\n","sns.countplot(\n","    x='Neighborhood',\n","    data=train_cleaned.sort_values('Neighborhood'),\n","    ax=axs[1]\n",")\n","\n","# Draw median price\n","axs[0].axhline(\n","    y=train_cleaned['SalePrice'].median(),\n","    color='red',\n","    linestyle='dotted'\n",")\n","\n","# Label the bars with counts\n","for patch in axs[1].patches:\n","    x = patch.get_bbox().get_points()[:, 0]\n","    y = patch.get_bbox().get_points()[1, 1]\n","    axs[1].annotate(f'{int(y)}', (x.mean(), y), ha='center', va='bottom')\n","\n","# Format x-axes\n","axs[1].set_xticklabels(axs[1].xaxis.get_majorticklabels(), rotation=90)\n","axs[0].xaxis.set_visible(False)\n","\n","# Narrow the gap between the plots\n","plt.subplots_adjust(hspace=0.01)"]},{"cell_type":"markdown","metadata":{"id":"ObeQQRzto6Et"},"source":["## Question 5a\n","Based on the plot above, what can be said about the relationship between the house sale prices and their neighborhoods?"]},{"cell_type":"markdown","metadata":{"id":"mfHKnZJDo6Et"},"source":["*Type your answer here, replacing this text.*"]},{"cell_type":"markdown","metadata":{"id":"9deD3rNOo6Et"},"source":["## Question 5b\n","One way we can deal with the lack of data from some neighborhoods is to create a new feature that bins neighborhoods together.  Let's categorize our neighborhoods in a crude way. Take the top 3 neighborhoods measured by median `SalePrice` and identify them as **\"rich neighborhoods\"**. We won't mark the other neighborhoods.\n","\n","Write a function that returns a list of the top `n` most pricy neighborhoods as measured by our choice of aggregating function (`np.median, np.mean, etc.`).  For example, in the setup above, we would want to call `find_rich_neighborhoods(train_cleaned, 3, np.median)` to find the top 3 neighborhoods measured by median `SalePrice`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yejd2xgKo6Et"},"outputs":[],"source":["def find_rich_neighborhoods(data, n=3, metric=np.median):\n","    \"\"\"\n","    Input:\n","      data (data frame): should contain at least a string-valued Neighborhood\n","        and a numeric SalePrice column\n","      n (int): the number of top values desired\n","      metric (function): function used for aggregating the data in each neighborhood.\n","        for example, np.median for median prices\n","\n","    Output:\n","      a list of the top n richest neighborhoods as measured by the metric function\n","    \"\"\"\n","\n","    # Write your code here.\n","    neighborhoods = ...\n","\n","    return neighborhoods\n","\n","# Find rich neighborhoodsv\n","rich_neighborhoods = find_rich_neighborhoods(train_cleaned, 3, np.median)\n","\n","# Print rich neighborhoods\n","print('The three richest neighborhoods are: {}'.format(rich_neighborhoods))"]},{"cell_type":"markdown","metadata":{"id":"yDOsKLVSo6Eu"},"source":["Check the figure above to make sure you've got the correct answer.\n","\n","## Question 5c\n","We now have a list of neighborhoods we've deemed as richer than others.  Let's use that information to make a new variable `in_rich_neighborhood`.  Write a function `add_rich_neighborhood` that adds an indicator variable which takes on the value 1 if the house is part of `rich_neighborhoods` and the value 0 otherwise.\n","\n","**Hint:** [`pd.Series.astype`](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.Series.astype.html) may be useful for converting True/False values to integers."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PP_ZS6Sdo6Eu"},"outputs":[],"source":["def add_in_rich_neighborhood(data, neighborhoods):\n","    \"\"\"\n","    Input:\n","      data (data frame): a data frame containing a 'Neighborhood' column with values\n","        found in the codebook\n","      neighborhoods (list of strings): strings should be the names of neighborhoods\n","        pre-identified as rich\n","    Output:\n","      data frame identical to the input with the addition of a binary\n","      in_rich_neighborhood column\n","    \"\"\"\n","    data_copy = data.copy()\n","\n","    # Write your code here.\n","    data_copy['in_rich_neighborhood'] = ...\n","\n","    return data_copy\n","\n","# Add 'in_rich_neighborhood' feature\n","train_cleaned_rich = add_in_rich_neighborhood(train_cleaned,\n","                                              rich_neighborhoods)\n","\n","# View DataFrame\n","train_cleaned_rich.head()"]},{"cell_type":"markdown","metadata":{"id":"d1oBL0W_o6Eu"},"source":["Let's check to see if our function added the new feature correctly. We should see a value of 1 for this rich neighborhood."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V1UJxWGeo6Eu"},"outputs":[],"source":["train_cleaned_rich[train_cleaned_rich['Neighborhood'] == 'NoRidge'].head()"]},{"cell_type":"markdown","metadata":{"id":"7cQqHoz4o6Eu"},"source":["## Fireplace Quality\n","In the following section, we will take a closer look at the Fireplace_Qu feature of the dataset and examine how we can incorporate categorical features into our linear model.\n","\n","## Question 5d\n","Let's see if our data set has any missing values.  Create a Series object containing the counts of missing values in each of the columns of our data set, sorted from greatest to least.  The Series should be indexed by the variable names.  For example, `missing_counts.loc['Fireplace_Qu']` should return 688.\n","\n","**Hint:** [`pandas.DataFrame.isnull()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isnull.html) may help here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sO8HGFlXo6Ev"},"outputs":[],"source":["# Write your code here.\n","missing_counts = ...\n","\n","# Display missing_counts\n","missing_counts"]},{"cell_type":"markdown","metadata":{"id":"-I8-iywio6Ev"},"source":["## Question 5e\n","An `NA` here actually means that the house had no fireplace to rate.  Let's fix this in our data set.  Write a function that replaces the missing values in `Fireplace_Qu` with `'No Fireplace'`.  In addition, it should replace each abbreviated condition with its full word.  For example, `'TA'` should be changed to `'Average'`, `'FA'` should be changed to `'Fair'`, and so on.  Hint: the [`DataFrame.replace()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html) method may be useful here."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rt71ZgN6o6Ev"},"outputs":[],"source":["def fix_fireplace_qu(data):\n","    \"\"\"\n","    Input:\n","      data (data frame): a data frame containing a Fireplace_Qu column.  Its values\n","                         should be limited to those found in the codebook\n","    Output:\n","      data frame identical to the input except with a refactored Fireplace_Qu column\n","    \"\"\"\n","\n","    # Write your code here.\n","    return ...\n","\n","# Correct Fireplace_Qu\n","training_data_fireplace_qu = fix_fireplace_qu(train_cleaned_rich)\n","\n","# View DataFrame\n","training_data_fireplace_qu.head()"]},{"cell_type":"markdown","metadata":{"id":"uCdy1Z13o6Ev"},"source":["Let's check out the unique values in `Fireplace_Qu` now."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfP_CcJGo6Ev"},"outputs":[],"source":["training_data_fireplace_qu['Fireplace_Qu'].unique().tolist()"]},{"cell_type":"markdown","metadata":{"id":"9Or_375No6Ev"},"source":["### Using Categorical Variables for Regression\n","Unfortunately, simply fixing these missing values isn't sufficient for using `Fireplace_Qu` in our model.  Since `Fireplace_Qu` is a categorical variable, we will have to **dummy-encode** the data. Note that dummy-encoding drops the first one-hot-encoded column. For more information on categorical data in pandas, refer to this [link](https://pandas-docs.github.io/pandas-docs-travis/categorical.html)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ReSoXX5Zo6Ev"},"outputs":[],"source":["def ohe_fireplace_qu(data):\n","    \"\"\"\n","    One-hot-encodes fireplace quality.  '\n","    New columns are of the form Fireplace_Qu_QUALITY\n","    \"\"\"\n","    # List categories\n","    categories = ['Excellent',\n","                  'Good',\n","                  'Average',\n","                  'Fair',\n","                  'Poor',\n","                  'No Fireplace']\n","\n","    category_type = pd.CategoricalDtype(categories=categories)\n","    data.loc[:, 'Fireplace_Qu'] = data.loc[:, 'Fireplace_Qu'].astype(category_type)\n","    data = pd.get_dummies(data,\n","                          prefix='Fireplace_Qu',\n","                          columns=['Fireplace_Qu'],\n","                          drop_first=True)\n","\n","    return data\n","\n","# Encode 'Fireplace_Qu'\n","training_data_ohe = ohe_fireplace_qu(training_data_fireplace_qu)\n","\n","# View new encoded features\n","training_data_ohe[[col for col in training_data_ohe.columns\n","                   if 'Fireplace_Qu' in col]].head()"]},{"cell_type":"markdown","metadata":{"id":"mii_01Zfo6Ew"},"source":["Notice that there are five new binary features:\n","- `'Fireplace_Qu_Good'`\n","- `'Fireplace_Qu_Average'`\n","- `'Fireplace_Qu_Fair'`\n","- `'Fireplace_Qu_Poor'`\n","- `'Fireplace_Qu_No Fireplace'`\n","\n","but we initially had 6 categories. Where is `'Fireplace_Qu_Excellent'`?\n","\n","Imagine a case where:\n","- `'Fireplace_Qu_Good'=0`\n","- `'Fireplace_Qu_Average'=0`\n","- `'Fireplace_Qu_Fair'=0`\n","- `'Fireplace_Qu_Poor'=0`\n","- `'Fireplace_Qu_No Fireplace'=0`\n","\n","This this would mean that the Fireplace quality is excellent. If we added a sixth features named `'Fireplace_Qu_Excellent'`, then is would mean that our Fireplace features would be correlated and redundant.\n","\n","Basically:\n","- `'Fireplace_Qu_Good'=0`\n","- `'Fireplace_Qu_Average'=0`\n","- `'Fireplace_Qu_Fair'=0`\n","- `'Fireplace_Qu_Poor'=0`\n","- `'Fireplace_Qu_No Fireplace'=0`\n","\n","would mean the same thing as:\n","- `'Fireplace_Qu_Excellent'=1`\n","\n","Therefore, it is desireable to dropped the first one-hot-encoded column, which we call dummy-encoding."]},{"cell_type":"markdown","metadata":{"id":"dCW3qaBXo6Ew"},"source":["# 6. Improving Our Linear Model\n","In this section, we will create linear models that produce more accurate estimates of the housing prices in Ames than the model created in the first half of this assgnment, but at the expense of increased complexity.\n","\n","The model we will fit can be written as follows:\n","\n","$$\n","\\text{SalePrice} =\n","\\theta_0 +\n","\\theta_1 \\cdot \\text{Gr}\\_\\text{Liv}\\_\\text{Area} +\n","\\theta_2 \\cdot \\text{Garage}\\_\\text{Area} +\n","\\theta_3 \\cdot \\text{total_bathrooms} +\n","\\theta_4 \\cdot \\text{in_rich_neighborhood} +\n","\\theta_5 \\cdot \\text{(Fireplace_Qu_Good)} +\n","\\theta_6 \\cdot \\text{(Fireplace_Qu_Average)} +\n","\\theta_7 \\cdot \\text{(Fireplace_Qu_Fair)} +\n","\\theta_8 \\cdot \\text{(Fireplace_Qu_Poor)} +\n","\\theta_9 \\cdot \\text{(Fireplace_Qu_No Fireplace)}\n","$$\n","\n","We still have a little bit of work to do prior to esimating our linear regression model's coefficients. Instead of having you go through the process of selecting the pertinent features and creating a [`sklearn.linear_model.LinearRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) object for our linear model again, we will provide the necessary code from the first half of this assignment. However, we will now use cross validation to help validate our model instead of explicitly splitting the data into a training and validation set.\n","\n","First, we will re-import the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MBZnuBgKo6Ew"},"outputs":[],"source":["ames_data_cleaned = pd.read_csv('/content/drive/MyDrive/Fall_2023/assignment1/ames_data_cleaned.csv')\n","ames_data_cleaned.head()"]},{"cell_type":"markdown","metadata":{"id":"rl0Cres_o6Ew"},"source":["And split into training and test data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-EaOA4-to6Ew"},"outputs":[],"source":["# Split dataset\n","train_cleaned, test_cleaned = train_test_split(ames_data_cleaned, test_size=0.30, random_state=0)\n","\n","# Print results\n","print('Train {}%'.format(train_cleaned.shape[0] / ames_data_cleaned.shape[0] * 100))\n","print('Test {}%'.format(test_cleaned.shape[0] / ames_data_cleaned.shape[0] * 100))"]},{"cell_type":"markdown","metadata":{"id":"XUKMtUx7o6Ew"},"source":["Next, we will implement a reusable pipeline that selects the required variables in our data and splits our feature and target variable into a matrix and a vector, respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQncXYXWo6Ex"},"outputs":[],"source":["def select_columns(data, *columns):\n","    \"\"\"Select only columns passed as arguments.\"\"\"\n","    return data.loc[:, columns]\n","\n","def process_data(data):\n","    \"\"\"Process the data for a guided model.\"\"\"\n","    # One-hot-encode fireplace quality feature\n","    data = fix_fireplace_qu(data)\n","    data = ohe_fireplace_qu(data)\n","\n","    # Use rich_neighborhoods computed earlier to add in_rich_neighborhoods feature\n","    data = add_in_rich_neighborhood(data, rich_neighborhoods)\n","\n","    # Transform Data, Select Features\n","    data = select_columns(data,\n","                          'SalePrice',\n","                          'Gr_Liv_Area',\n","                          'Garage_Area',\n","                          'total_bathrooms',\n","                          'in_rich_neighborhood',\n","                          'Fireplace_Qu_Good',\n","                          'Fireplace_Qu_Average',\n","                          'Fireplace_Qu_Fair',\n","                          'Fireplace_Qu_Poor',\n","                          'Fireplace_Qu_No Fireplace')\n","\n","    # Return predictors and response variables separately\n","    X = data.drop(['SalePrice'], axis = 1)\n","    y = data.loc[:, 'SalePrice']\n","\n","    return X, y"]},{"cell_type":"markdown","metadata":{"id":"TDUmJHCgo6Ex"},"source":["We then process our training set using our data cleaning pipeline."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9QGI9UBpo6Ex"},"outputs":[],"source":["# Pre-process the training data\n","# Our functions make this very easy!\n","X_train, y_train = process_data(train_cleaned)\n","X_train.head()"]},{"cell_type":"markdown","metadata":{"id":"etNgRv87o6Ex"},"source":["## Question 6a\n","Use the `cross_validate_rmse` function to calculate the cross validation error in the cell below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7d5_UaOoo6Ex"},"outputs":[],"source":["# Write your code here.\n","cv_scores_updated = ...\n","\n","# Print cv scores\n","print('Cross-validation RMSE scores: {}'.format(cv_scores_updated))\n","print('Cross-validation RMSE scores mean: ${}'.format(np.mean(cv_scores_updated)))\n","print('Cross-validation RMSE scores std: ${}'.format(np.std(cv_scores_updated)))"]},{"cell_type":"markdown","metadata":{"id":"MVNP1MBUo6Ex"},"source":["Let's compare this to our earlier cross-validation score when only using:\n","- `'SalePrice'`\n","- `'Gr_Liv_Area'`\n","- `'Garage_Area'`\n","- `'total_bathrooms'`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nfr5UjE5o6Ex"},"outputs":[],"source":["print('Cross-validation RMSE scores mean: ${}'.format(np.mean(cv_scores)))"]},{"cell_type":"markdown","metadata":{"id":"KSIlCXKlo6Ey"},"source":["You've done it! by adding two new features, we've improved our model's performance.\n","\n","Now that we are happy with out model's performance and have settled on a final set of features, we can train the final model on the entire training dataset. First, we initialize a `sklearn.linear_model.LinearRegression()` object as our linear model. We set the `fit_intercept=True` to ensure that the linear model has a non-zero intercept."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vDPJ2HiBo6Ey"},"outputs":[],"source":["from sklearn.linear_model import LinearRegression\n","\n","linear_model = LinearRegression(fit_intercept=True)"]},{"cell_type":"markdown","metadata":{"id":"lhR4cOjRo6Ey"},"source":["It's finally time to fit our updated linear regression model. The cell below estimates the model and then uses it to compute the fitted value of `SalePrice` over the training data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mDOZlqp_o6Ey"},"outputs":[],"source":["# Fit the model\n","linear_model.fit(X_train, y_train)\n","\n","# Compute the fitted and predicted values of SalePrice\n","y_fitted = linear_model.predict(X_train)"]},{"cell_type":"markdown","metadata":{"id":"C8tvYKKNo6Ey"},"source":["Let's assess the performance of our new linear regression model using the Root Mean Squared Error function from earlier in this assignment."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XrjNtm19o6Ey"},"outputs":[],"source":["training_error = rmse(y_fitted, y_train)\n","print(\"Training RMSE: ${}\".format(training_error))"]},{"cell_type":"markdown","metadata":{"id":"wKyKCuMNo6Ez"},"source":["## Question 6b\n","Now that we have trained our final model, we can evaluate it on our test data. Prediced. Predict the house price for the test feature `X_test` and named the variable `y_predicted`. Then compute the test `RMSE`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ha6V_SEOo6Ez"},"outputs":[],"source":["# Pre-process the training data\n","# Our functions make this very easy!\n","X_test, y_test = process_data(test_cleaned)\n","X_test.head()\n","\n","# Write your code here.\n","y_predicted = ...\n","test_error = ...\n","print(\"Test RMSE: ${}\".format(test_error))"]},{"cell_type":"markdown","metadata":{"id":"IHn0ADSPo6Ez"},"source":["# 7. Open-Response\n","The following part is purposefully left nearly open-ended.\n","\n","## Question 7\n","Your goal is to provide a linear regression model that improves the cross-validation root mean square error from the previous section.\n","\n","- Cross-validation RMSE scores mean: `$39006.42198732712`\n","\n","To do this, you should add at least one new feature. Please use Markdown cells to explain your thinking when engineering new features.\n","\n","Let's import the data and split again with new variable names for this section."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oAN4csoho6Ez"},"outputs":[],"source":["ames_data_cleaned_q7 = pd.read_csv('/content/drive/MyDrive/Fall_2023/assignment1/ames_data.csv')\n","ames_data_cleaned_q7.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mWgXEA9ao6Ez"},"outputs":[],"source":["# Split dataset\n","train_cleaned_q7, test_cleaned_q7 = train_test_split(ames_data_cleaned_q7, test_size=0.30, random_state=0)\n","\n","# Print results\n","print('Train {}%'.format(train_cleaned_q7.shape[0] / ames_data_cleaned_q7.shape[0] * 100))\n","print('Test {}%'.format(test_cleaned_q7.shape[0] / ames_data_cleaned_q7.shape[0] * 100))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GXBTq1GTo6Ez"},"outputs":[],"source":["# Write your code here. (Add as many new cells as you like)"]},{"cell_type":"markdown","metadata":{"id":"viMXeS3No6Ez"},"source":["Here is your resuable pipeline. You'll want to add a least one new feature here:\n","\n","```python\n","data = select_columns(data,\n","                      'SalePrice',\n","                      'Gr_Liv_Area',\n","                      'Garage_Area',\n","                      'total_bathrooms',\n","                      'in_rich_neighborhood',\n","                      'Fireplace_Qu_Good',\n","                      'Fireplace_Qu_Average',\n","                      'Fireplace_Qu_Fair',\n","                      'Fireplace_Qu_Poor',\n","                      'Fireplace_Qu_No Fireplace')\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u_HwkcApo6E0"},"outputs":[],"source":["def select_columns(data, *columns):\n","    \"\"\"Select only columns passed as arguments.\"\"\"\n","    return data.loc[:, columns]\n","\n","def process_data(data):\n","    \"\"\"Process the data for a guided model.\"\"\"\n","    # One-hot-encode fireplace quality feature\n","    data = fix_fireplace_qu(data)\n","    data = ohe_fireplace_qu(data)\n","\n","    # Use rich_neighborhoods computed earlier to add in_rich_neighborhoods feature\n","    data = add_in_rich_neighborhood(data, rich_neighborhoods)\n","\n","    # Transform Data, Select Features\n","    data = select_columns(data,\n","                          'SalePrice',\n","                          'Gr_Liv_Area',\n","                          'Garage_Area',\n","                          'total_bathrooms',\n","                          'in_rich_neighborhood',\n","                          'Fireplace_Qu_Good',\n","                          'Fireplace_Qu_Average',\n","                          'Fireplace_Qu_Fair',\n","                          'Fireplace_Qu_Poor',\n","                          'Fireplace_Qu_No Fireplace')\n","\n","    # Return predictors and response variables separately\n","    X = data.drop(['SalePrice'], axis = 1)\n","    y = data.loc[:, 'SalePrice']\n","\n","    return X, y"]},{"cell_type":"markdown","metadata":{"id":"kHKB7Wcfo6E0"},"source":["## Final Evaluation\n","This is where you can compute your cross-validation score. `X_train_q7` and `y_train_q7` should be your new feature and target variables. `X_train_q7` should include at least one new feature."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LL-ZtVjvo6E0"},"outputs":[],"source":["# Pre-process the training data\n","X_train_q7, y_train_q7 = process_data(train_cleaned_q7)\n","\n","# Write your code here.\n","cv_scores_updated = cross_validate_rmse(model=lm.LinearRegression(fit_intercept=True),\n","                                        X=X_train_q7, y=y_train_q7)\n","\n","# Print cv scores\n","print('Cross-validation RMSE scores: {}'.format(cv_scores_updated))\n","print('Cross-validation RMSE scores mean: ${}'.format(np.mean(cv_scores_updated)))\n","print('Cross-validation RMSE scores std: ${}'.format(np.std(cv_scores_updated)))"]},{"cell_type":"markdown","metadata":{"id":"gWWLR1q8o6E0"},"source":["**Congratulation, you're done Assignment 1. Review your answers and clean up that code before submitting on Dropbox. `#cleancode`**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QiWPjjfUtLxF"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
