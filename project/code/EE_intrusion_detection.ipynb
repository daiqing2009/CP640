{"metadata":{"kernelspec":{"display_name":"venv640","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"},"colab":{"include_colab_link":true,"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6425404,"sourceType":"datasetVersion","datasetId":3706998},{"sourceId":10124308,"sourceType":"datasetVersion","datasetId":6247537}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":236.319221,"end_time":"2024-12-07T15:51:11.975723","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-12-07T15:47:15.656502","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/daiqing2009/effective-and-efficient-intrusion-detection?scriptVersionId=212414383\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"<a href=\"https://www.kaggle.com/code/daiqing2009/effective-and-efficient-intrusion-detection?scriptVersionId=211743249\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{}},{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/daiqing2009/CP640/blob/main/project/code/EE_intrusion_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","papermill":{"duration":0.011767,"end_time":"2024-12-07T15:47:18.885257","exception":false,"start_time":"2024-12-07T15:47:18.87349","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Table of Contents\nThe goal of this project to measure the effectiveness of common machine learning techniques in differentiating malicious traffic from benign one and identifying corresponding attacks categories and sub-categories. Meanwhile, the project will challenge the requirement to detect the intrusion traffic efficiently as required in real-world situations, especially how models trained on small data perform on larger data.\n* Effective Machine Learning\n    * Import isproportional Data sampling\n    * EDA & Preprocessing\n    * Model Selection and Evaluation\n* Efficient Machine Learning\n    * Model performance on different scale of sampling data\n    * Potential integration with IPS(Intrusion Prevention System)\n","metadata":{"id":"762f69d5","papermill":{"duration":0.010374,"end_time":"2024-12-07T15:47:18.906464","exception":false,"start_time":"2024-12-07T15:47:18.89609","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Effective Machine Learning\nIn this section, dataaset will be imported. Then Exploratory Data Analysis(EDA) and corresponding preprocessing will conducted before model training and evaluation.","metadata":{"id":"924b06ed","papermill":{"duration":0.010398,"end_time":"2024-12-07T15:47:18.927473","exception":false,"start_time":"2024-12-07T15:47:18.917075","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import sklearn as sk\nimport seaborn as sns\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, accuracy_score, recall_score, precision_score, f1_score\n\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nRANDOM_STATE = 42\n# for lib in [pd, sk, sns, sv, optuna, tf, keras]:\nfor lib in [pd, sk, sns]:\n    print(f\"Using {lib.__name__} {lib.__version__}\")\n\n# define the thresthold for proprocessors\nVAR_THR = 0.025\nCORR_THR = 0.93","metadata":{"ExecuteTime":{"end_time":"2024-08-21T11:48:20.817741Z","start_time":"2024-08-21T11:47:39.157249Z"},"execution":{"iopub.execute_input":"2024-12-07T15:47:18.953612Z","iopub.status.busy":"2024-12-07T15:47:18.953263Z","iopub.status.idle":"2024-12-07T15:47:21.941244Z","shell.execute_reply":"2024-12-07T15:47:21.940013Z"},"id":"a209c7ec","outputId":"21092241-46c8-4460-a58b-49454e82d002","papermill":{"duration":3.004274,"end_time":"2024-12-07T15:47:21.943747","exception":false,"start_time":"2024-12-07T15:47:18.939473","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Importing Data\nThe importing subsection try to be competible in all environments, including Colab, Kaggle and local dev. The original dataset is on https://www.kaggle.com/datasets/daiqing2009/ciciot2023-disprop-sampling","metadata":{"id":"31e3e84a","papermill":{"duration":0.010741,"end_time":"2024-12-07T15:47:21.968739","exception":false,"start_time":"2024-12-07T15:47:21.957998","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import os\n\ndef detect_environment():\n    \"\"\"Detects the environment: Kaggle, Google Colab, or Local.\"\"\"\n    if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ:\n        return \"Kaggle\"\n    elif \"COLAB_GPU\" in os.environ:\n        return \"Google Colab\"\n    else:\n        return \"Local\"\n\ndef create_folder_if_not_exists(path):\n    \"\"\"Creates a folder if it does not already exist.\"\"\"\n    if not os.path.exists(path):\n        os.makedirs(path)\n        print(f\"Folder created at: {path}\")\n    else:\n        print(f\"Folder already exists at: {path}\")\n\ndef setup_kaggle_credentials():\n    \"\"\"Ensures Kaggle API credentials are available.\"\"\"\n    if detect_environment() == \"Google Colab\":\n      from google.colab import userdata\n      os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n      os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n    elif detect_environment() == \"Local\":\n      kaggle_json_path = os.path.expanduser(\"~/.kaggle/kaggle.json\")\n      if not os.path.exists(kaggle_json_path):\n        # Prompt user to upload `kaggle.json`\n        print(\"Please upload your `kaggle.json` file.\")\n        from google.colab import files\n        uploaded = files.upload()\n        with open(kaggle_json_path, \"wb\") as f:\n            f.write(uploaded['kaggle.json'])\n      os.chmod(kaggle_json_path, 0o600)  # Set correct permissions for the file\n\ndef download_kaggle_resource(input_path, resource_name):\n    \"\"\"\n    Downloads a dataset or notebook output from Kaggle using the resource name.\n    Automatically unzips if needed and logs useful diagnostic information.\n    \"\"\"\n    from kaggle.api.kaggle_api_extended import KaggleApi\n    import os\n\n    setup_kaggle_credentials()\n\n    # Initialize Kaggle API\n    api = KaggleApi()\n    api.authenticate()\n\n    try:\n        # Attempt to download as a dataset\n        print(f\"Attempting to download dataset: {resource_name}\")\n        api.dataset_download_files(resource_name, path=input_path, unzip=True)\n        print(f\"Dataset downloaded and extracted to {input_path}\")\n    except Exception as dataset_error:\n        print(f\"Dataset not found or inaccessible: {dataset_error}. Attempting as notebook output.\")\n        try:\n            # Attempt to download as notebook output\n            print(f\"Attempting to download notebook output: {resource_name}\")\n            api.kernel_output(resource_name, path=input_path, unzip=True)\n            print(f\"Notebook output downloaded and extracted to {input_path}\")\n        except Exception as notebook_error:\n            print(f\"Failed to download resource: {resource_name}.\")\n            print(f\"Dataset error: {dataset_error}\")\n            print(f\"Notebook output error: {notebook_error}\")\n            print(\"Please verify the resource name, access permissions, and your Kaggle API credentials.\")\n\n# Step 1: Detect environment\nenvironment = detect_environment()\nprint(f\"Environment detected: {environment}\")\n\n# Step 2: Create input folder\nif environment == \"Kaggle\":\n    input_path = \"/kaggle/input\"\nelif environment == \"Google Colab\":\n    input_path = \"/content/input\"\nelif environment == \"Local\":\n    input_path = \"./input\"\nelse:\n    raise ValueError(\"Unknown environment detected!\")\n\ncreate_folder_if_not_exists(input_path)\nprint(f\"Input folder path: {input_path}\")\n\n# Step 3: Create output folder\nif environment == \"Kaggle\":\n    output_path = \"/kaggle/working/output\"\nelif environment == \"Google Colab\":\n    output_path = \"/content/output\"\nelif environment == \"Local\":\n    output_path = \"./output\"\nelse:\n    raise ValueError(\"Unknown environment detected!\")\n\ncreate_folder_if_not_exists(output_path)\nprint(f\"Output folder path: {output_path}\")\n\nresource_name = \"daiqing2009/ciciot2023-disprop-sampling\"\n# Step 4: Download dataset or notebook output\nif environment != \"Kaggle\":\n    pass\n    # resource_name = input(\"Enter the Kaggle dataset name or notebook name to download: \")\n    download_kaggle_resource(input_path, resource_name)\n\n# initialize the data prefix\nif environment == \"Kaggle\":\n    dataprefix = input_path + \"/\" + resource_name.split(\"/\")[1]\nelif environment == \"Google Colab\":\n    dataprefix = input_path\nelif environment == \"Local\":\n    dataprefix = input_path\n# mount cloud drive if necessary\nif environment == \"Google Colab\":\n  from google.colab import drive\n  drive.mount('/content/drive')","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:21.993336Z","iopub.status.busy":"2024-12-07T15:47:21.992766Z","iopub.status.idle":"2024-12-07T15:47:22.010836Z","shell.execute_reply":"2024-12-07T15:47:22.009686Z"},"id":"553a0b2d","outputId":"52a50044-09ce-4f7b-c6e5-b1ecb12812c9","papermill":{"duration":0.033155,"end_time":"2024-12-07T15:47:22.013005","exception":false,"start_time":"2024-12-07T15:47:21.97985","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 0.1 percent as size xs\ndf2_xs = pd.read_csv(f\"{dataprefix}/0.001_dist_percent_2classes.csv\")\ndf8_xs = pd.read_csv(f\"{dataprefix}/0.001_dist_percent_8classes.csv\")\ndf34_xs = pd.read_csv(f\"{dataprefix}/0.001_dist_percent_34classes.csv\")\n\n# 0.5 percent as size s\ndf2_s = pd.read_csv(f\"{dataprefix}/0.005_dist_percent_2classes.csv\")\ndf8_s = pd.read_csv(f\"{dataprefix}/0.005_dist_percent_8classes.csv\")\ndf34_s = pd.read_csv(f\"{dataprefix}/0.005_dist_percent_34classes.csv\")\n\n# 1 percent as size m\n# df2_m = pd.read_csv(\"/kaggle/input/disproportionate-sampling-dataset-for-ciciot2023/0.01_dist_percent_2classes.csv\")\ndf8_m = pd.read_csv(f\"{dataprefix}/0.01_dist_percent_8classes.csv\")\n# df34_m = pd.read_csv(\"/kaggle/input/disproportionate-sampling-dataset-for-ciciot2023/0.01_dist_percent_34classes.csv\")","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:22.037649Z","iopub.status.busy":"2024-12-07T15:47:22.037286Z","iopub.status.idle":"2024-12-07T15:47:31.560551Z","shell.execute_reply":"2024-12-07T15:47:31.559566Z"},"id":"7d19a433","papermill":{"duration":9.538102,"end_time":"2024-12-07T15:47:31.563009","exception":false,"start_time":"2024-12-07T15:47:22.024907","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# setting the default dataset\ndf2 = df2_xs\ndf8 = df8_xs\ndf34 = df34_xs","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:31.587645Z","iopub.status.busy":"2024-12-07T15:47:31.58729Z","iopub.status.idle":"2024-12-07T15:47:31.591975Z","shell.execute_reply":"2024-12-07T15:47:31.590968Z"},"id":"0df4094c","papermill":{"duration":0.019376,"end_time":"2024-12-07T15:47:31.59423","exception":false,"start_time":"2024-12-07T15:47:31.574854","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# extract the label value\nlabel2=df2[\"benign\"]\ndf2 = df2.drop(\"benign\", axis=1)\nlabel8=df8[\"label\"]\ndf8 = df8.drop(\"label\", axis=1)\nlabel34=df34[\"label\"]\ndf34 = df34.drop(\"label\", axis=1)","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:31.618353Z","iopub.status.busy":"2024-12-07T15:47:31.617975Z","iopub.status.idle":"2024-12-07T15:47:31.646833Z","shell.execute_reply":"2024-12-07T15:47:31.645559Z"},"id":"c57131f5","papermill":{"duration":0.043799,"end_time":"2024-12-07T15:47:31.649196","exception":false,"start_time":"2024-12-07T15:47:31.605397","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA & Preprocessing\n**Summary Table of EDA Methods and Preprocessing**\nFollowing table is a brief summary of common Exploratory Data Analysis (EDA) and corresponding Preprocessing methods.\n\n| EDA Method|Typical Preprocessing/Feature Engineering|Relevancy to current probelm|\n|--|--|--|\n|Data Cleaning|Handle missing values, duplicates, and outliers.|outlier is the challenge of the dataset, capping techniques are applied to |\n|Univariate Analysis|Normalize/encode features, transform skewed data.|show skewness of the data and visualize with histogram, boxplot, drop column with limited info|\n|Multivariate Analysis|Normalize, reduce multicollinearity, apply dimensionality reduction.|show Correlation table and reduce dimension with PCA/t-CNS|\n|Target Variable Analysis|Balance classes, transform skewed distributions.|the label class is highly biased , espcially for (D)DOS. Dispropotional Sampling is used to balance class |\n|Time Series Analysis|Interpolate missing data, extract date/time features, apply smoothing.|N/A, since the dataset is already engineered and temporal information has been compacted|\nThe preprocessing will be conducted in following steps:\n\n\n1. OneHotEncoding for categorical features.\n2. Drop features of both low variance and low correlation with target label.\n3. Drop highly correlated features\n4. Reduce dimension Using PCA\n\nFinally, all these steps are pipelined using scikit learn functions.\n\n","metadata":{"id":"752101f2","papermill":{"duration":0.0116,"end_time":"2024-12-07T15:47:31.67211","exception":false,"start_time":"2024-12-07T15:47:31.66051","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# Calculate percentages for each unique value\nvalue_counts = df2['protocol_type'].value_counts(normalize=True) * 100\n\n# Sort percentages from highest to lowest\nsorted_percentages = value_counts.sort_values(ascending=False)\n\n# Get top 10 percentages (or fewer if there are less than unique values)\ntop_percentages = sorted_percentages.head(10)\n\n# Plot histogram for the top values sorted by percentage\nplt.bar(top_percentages.index, top_percentages.values)\nplt.title(\"Top Percentages Distribution (Highest to Lowest)\")\nplt.xlabel(\"protocol_type\")\nplt.ylabel(\"Percentage (%)\")\nplt.xticks(top_percentages.index)  # Ensure x-axis labels show only top values\nplt.show()\n\n# Print the top sorted percentages\nprint(\"Top 20 Percentages (from Highest to Lowest):\")\nprint(sorted_percentages[:20])","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:31.695957Z","iopub.status.busy":"2024-12-07T15:47:31.695609Z","iopub.status.idle":"2024-12-07T15:47:31.986462Z","shell.execute_reply":"2024-12-07T15:47:31.985468Z"},"id":"f813ce86","jupyter":{"source_hidden":true},"outputId":"a8d6f44e-875c-45c3-aa85-58d76e16ef8c","papermill":{"duration":0.306645,"end_time":"2024-12-07T15:47:31.98983","exception":false,"start_time":"2024-12-07T15:47:31.683185","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: OneHotEncoding for categorical features\ncategorical_features = ['protocol_type']\ncategorical_transformer = OneHotEncoder(sparse_output=False)\nencoded_categorical = categorical_transformer.fit_transform(df2[categorical_features])\n# print(encoded_categorical)\n\n# Create a DataFrame for the encoded categorical features\nencoded_df = pd.DataFrame(encoded_categorical, columns=categorical_transformer.get_feature_names_out(categorical_features))\n\n# Drop the original categorical columns and concatenate the encoded features\ndf2_filtered = df2.drop(categorical_features, axis=1)\ndf2_filtered = pd.concat([df2_filtered, encoded_df], axis=1)\ndf2_filtered.head()","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:32.015094Z","iopub.status.busy":"2024-12-07T15:47:32.014123Z","iopub.status.idle":"2024-12-07T15:47:32.08186Z","shell.execute_reply":"2024-12-07T15:47:32.080704Z"},"id":"3ca4b0e7","outputId":"6b83cf8d-362a-4675-af8c-80558e194a38","papermill":{"duration":0.082793,"end_time":"2024-12-07T15:47:32.084253","exception":false,"start_time":"2024-12-07T15:47:32.00146","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"onehot_cols = ['http','https','dns','telnet','smtp','ssh','irc','tcp','udp','dhcp','arp','icmp','ipv','llc']\nonehot_cols.extend(encoded_df.columns.tolist())\n# print(f'onehot_cols:{onehot_cols}')\nfrom scipy.stats import chi2_contingency\n\ndef cramers_v(contingency_table):\n    \"\"\"Calculate Cramér's V for a contingency table.\"\"\"\n    chi2, _, _, _ = chi2_contingency(contingency_table)\n    n = contingency_table.sum().sum()\n    return np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n\ndef filter_strong_association_features(df, target, threshold=0.3):\n    \"\"\"\n    Filter binary/categorical features strongly associated with a categorical target label.\n\n    Parameters:\n        df (pd.DataFrame): DataFrame containing features and the target label.\n        target_col (str): Name of the categorical target label column.\n        threshold (float): Minimum Cramér's V value to consider a strong association.\n\n    Returns:\n        list: List of feature names with strong association.\n    \"\"\"\n    strong_features = []\n\n    # Loop through features in the DataFrame\n    for col in df.columns:\n        # if col == target_col:  # Skip the target column\n        #     continue\n        if df[col].nunique() <= 10:  # Check if the column is categorical/binary\n            # Create a contingency table\n            contingency_table = pd.crosstab(df[col], target)\n\n            # Calculate Cramér's V\n            cramer_v_value = cramers_v(contingency_table)\n\n            # If Cramér's V exceeds the threshold, consider it strongly associated\n            if cramer_v_value >= threshold:\n                strong_features.append(col)\n\n    return strong_features\n\ncols_valueable = filter_strong_association_features(df2_filtered,label34, threshold=0.3)\nprint(f'cols_valueable in onehotcode:{cols_valueable}')","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:32.109932Z","iopub.status.busy":"2024-12-07T15:47:32.10955Z","iopub.status.idle":"2024-12-07T15:47:32.981684Z","shell.execute_reply":"2024-12-07T15:47:32.979976Z"},"id":"c1c63c40","outputId":"b81d55bf-61cd-4a4d-f8e5-937cb839a3f9","papermill":{"duration":0.888467,"end_time":"2024-12-07T15:47:32.984752","exception":false,"start_time":"2024-12-07T15:47:32.096285","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# filter out columns with uniform distribution(always the same value)\nskewness = df2_filtered.skew()\n# print(skewness)\n\ndf2_ls = df2_filtered.loc[:,skewness<0.00001]\n\n# Create a grid of subplots\nfig, axes = plt.subplots(3, 3, figsize=(9, 9))  # 3 rows, 3 columns\naxes = axes.flatten()  # Flatten the axes array for easy indexing\n\n# Plot each column's histogram in the corresponding subplot\nfor i, column in enumerate(df2_ls.columns):\n    # Count the occurrences of True and False\n    value_counts = df2_ls[column].value_counts()\n    # Plot the histogram\n    value_counts.plot(kind='bar', color=['skyblue', 'orange'], ax=axes[i])\n    axes[i].set_title(f'{column}')\n    axes[i].set_xlabel('Value')\n    axes[i].set_ylabel('Count')\n    axes[i].set_xticks([0, 1])\n    axes[i].set_xticklabels(['False', 'True'], rotation=0)\n\n# Adjust layout for better spacing\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:33.011381Z","iopub.status.busy":"2024-12-07T15:47:33.010988Z","iopub.status.idle":"2024-12-07T15:47:34.416059Z","shell.execute_reply":"2024-12-07T15:47:34.414846Z"},"id":"30e8c59d","outputId":"532ced4b-3e85-4ac9-ff56-bd46538303e5","papermill":{"duration":1.421147,"end_time":"2024-12-07T15:47:34.418613","exception":false,"start_time":"2024-12-07T15:47:32.997466","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# analyze the colunmn with high abs skew values\ndf2_hs = df2_filtered.loc[:,abs(skewness)>50]\n\n# print(df2_hs.head(10))\n\n# # Identify continuous columns (e.g., numeric columns)\ncontinuous_columns = df2_hs.select_dtypes(include=['float64', 'int64']).columns\n\n# Set up the plotting\nplt.figure(figsize=(10, 10))\n\n# delete columns with too high skewness\nfrom scipy.stats import mstats\n\ndf_winsorized = df2_hs[continuous_columns].apply(lambda x: mstats.winsorize(x, limits=[0.05, 0.05]))\n\n# Loop through continuous columns and plot histograms\nfor i, col in enumerate(continuous_columns, 1):\n    plt.subplot(5,5, i)\n    sns.histplot(df_winsorized[col], kde=True, bins=10, color='skyblue', edgecolor='black')  # histogram with KDE\n    plt.title(f'{col}')\n    plt.xlabel(col)\n    plt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:34.446915Z","iopub.status.busy":"2024-12-07T15:47:34.446553Z","iopub.status.idle":"2024-12-07T15:47:38.122434Z","shell.execute_reply":"2024-12-07T15:47:38.121324Z"},"id":"4bd1e46b","outputId":"c542b0c3-6b7c-476a-aa48-1ecfc88f8da5","papermill":{"duration":3.692685,"end_time":"2024-12-07T15:47:38.124707","exception":false,"start_time":"2024-12-07T15:47:34.432022","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: drop features of both low variance and low correlation with target label.\nvar_thr = VarianceThreshold(threshold = VAR_THR) #Removing both constant and quasi-constant\nvar_thr.fit(df2_filtered)\n\nvar_thr.get_support()\nconcol = [column for column in df2_filtered.columns\n          if column not in df2_filtered.columns[var_thr.get_support()]]\nprint(f'columns of low variance: {concol}')","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:38.15604Z","iopub.status.busy":"2024-12-07T15:47:38.155631Z","iopub.status.idle":"2024-12-07T15:47:38.213621Z","shell.execute_reply":"2024-12-07T15:47:38.212507Z"},"id":"6caa87fa","outputId":"5e459bfd-9dd9-4655-c4d9-f253a625f369","papermill":{"duration":0.076164,"end_time":"2024-12-07T15:47:38.215913","exception":false,"start_time":"2024-12-07T15:47:38.139749","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# remove the feature of both low variance and low\ncols_to_drop = [col for col in concol if col not in cols_valueable]\nprint(f'cols_to_drop: {cols_to_drop}')\ndf2_normal = df2_filtered.drop(concol,axis =1)\ndf2_normal.head()","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:38.247265Z","iopub.status.busy":"2024-12-07T15:47:38.246843Z","iopub.status.idle":"2024-12-07T15:47:38.274747Z","shell.execute_reply":"2024-12-07T15:47:38.2737Z"},"id":"749cfdbf","outputId":"af310ae0-24df-4f43-dda9-824697c19eab","papermill":{"duration":0.046264,"end_time":"2024-12-07T15:47:38.276851","exception":false,"start_time":"2024-12-07T15:47:38.230587","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show boxplot of parameter with comparatively normal distribution\ncontinuous_columns = df2_normal.select_dtypes(include=['float64', 'int64']).columns\n\n# Create a grid for the boxplots\nplt.figure(figsize=(12, 12))\n\n# Loop through each column and plot the boxplot\nfor i, col in enumerate(continuous_columns, 1):\n    plt.subplot(6,6, i)\n    sns.boxplot(x=df2_normal[col])\n    plt.title(f'{col}')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:38.308575Z","iopub.status.busy":"2024-12-07T15:47:38.308207Z","iopub.status.idle":"2024-12-07T15:47:41.7391Z","shell.execute_reply":"2024-12-07T15:47:41.738005Z"},"id":"88cba9a8","outputId":"093eeead-8846-445b-fe99-c00ded19e81f","papermill":{"duration":3.450732,"end_time":"2024-12-07T15:47:41.742466","exception":false,"start_time":"2024-12-07T15:47:38.291734","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Correlation Analysis\ndef show_corr(df):\n    corr_matrix = df.corr()\n\n    # Visualize the correlation matrix\n\n    plt.figure(figsize=(20, 20))\n\n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.xticks(rotation=90)\n    plt.title('Correlation Matrix')\n    plt.show()\nshow_corr(df2_normal)","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:41.778674Z","iopub.status.busy":"2024-12-07T15:47:41.778276Z","iopub.status.idle":"2024-12-07T15:47:45.622401Z","shell.execute_reply":"2024-12-07T15:47:45.621207Z"},"id":"dbb544d3","outputId":"19ade289-ef44-4920-9791-38785e1145a7","papermill":{"duration":3.873589,"end_time":"2024-12-07T15:47:45.632971","exception":false,"start_time":"2024-12-07T15:47:41.759382","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Drop highly correlated features\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass CorrelationFilter(BaseEstimator, TransformerMixin):\n    def __init__(self, threshold=0.9):\n        self.threshold = threshold\n        self.to_drop_ = None\n\n    def fit(self, X, y=None):\n        # Ensure X is a DataFrame\n        X = pd.DataFrame(X)\n\n        # Compute the correlation matrix\n        corr_matrix = X.corr().abs()\n\n        # Identify highly correlated features\n        upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n        self.to_drop_ = [column for column in upper_triangle.columns if any(upper_triangle[column] > self.threshold)]\n        print(f'column of high correlation({self.threshold}) to be dropped: {self.to_drop_}')\n        return self\n\n    def transform(self, X):\n        # Ensure X is a DataFrame\n        X = pd.DataFrame(X)\n\n        # Drop identified columns\n        return X.drop(columns=self.to_drop_, errors='ignore')\n\n    def fit_transform(self, X, y=None):\n        self.fit(X, y)\n        return self.transform(X)\ndf_reduced = CorrelationFilter(threshold=CORR_THR).fit_transform(df2_normal)\n# df_reduced = CorrelationFilter(threshold=0.9).fit_transform(df2_normal)","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:45.696532Z","iopub.status.busy":"2024-12-07T15:47:45.69509Z","iopub.status.idle":"2024-12-07T15:47:45.902027Z","shell.execute_reply":"2024-12-07T15:47:45.900971Z"},"id":"6bf07256","outputId":"e771845f-22c5-4c4f-a697-43d3cf3979be","papermill":{"duration":0.241624,"end_time":"2024-12-07T15:47:45.904317","exception":false,"start_time":"2024-12-07T15:47:45.662693","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step4:reduce dimension Using PCA\nfrom sklearn.decomposition import PCA\n\ndef CumuPlot(df):\n    # Apply PCA with the adjusted number of components\n    pca = PCA()\n\n    df_pca = pca.fit_transform(df)\n\n    # Calculate cumulative variance\n    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n\n    # Plot cumulative explained variance\n    plt.figure(figsize=(10, 8))\n    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-', color='b')\n    plt.axhline(y=0.90, color='m', linestyle='-.', label='90% Threshold')  # Optional threshold line\n    plt.axhline(y=0.95, color='r', linestyle='--', label='95% Threshold')  # Optional threshold line\n    plt.xticks(range(1, len(cumulative_variance) + 1))  # Ensure x-ticks align with components\n    plt.xlabel('Number of Principal Components')\n    plt.ylabel('Cumulative Explained Variance')\n    plt.title('Cumulative Explained Variance by PCA Components')\n    plt.legend()\n    plt.grid()\n    plt.show()\n\nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df_reduced)\nCumuPlot(df_scaled)","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:45.967057Z","iopub.status.busy":"2024-12-07T15:47:45.966626Z","iopub.status.idle":"2024-12-07T15:47:46.653178Z","shell.execute_reply":"2024-12-07T15:47:46.652026Z"},"id":"f3df6225","outputId":"7143d98c-4d97-4696-e255-383e7c6f8ce1","papermill":{"duration":0.721167,"end_time":"2024-12-07T15:47:46.655941","exception":false,"start_time":"2024-12-07T15:47:45.934774","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply PCA and retain the top n components\nn_components = 20 #take 95% variance as target\npca = PCA(n_components=n_components)\n\ndf_scaled = pd.DataFrame(df_scaled)\nprincipal_components = pca.fit_transform(df_scaled)\n\n# print(scaler.fit_transform(principal_components)[:5])\n\n# Get PCA components (loadings)\ncomponents = pca.components_\n\ndf = df_reduced\n# Compute the importance of each feature\nimportance = components.T  # Transpose to make features the rows\nimportance_scores = pd.DataFrame(importance, columns=[f'PC{i+1}' for i in range(components.shape[0])], index=df.columns)\n\n# Calculate the total contribution of each feature across all PCs\ntop_n = 20  # Change this to the desired number of features\nimportance_scores['Total'] = (importance_scores.abs() * pca.explained_variance_ratio_).sum(axis=1)\nprint(f\"Top({top_n}) features of  importance: \")\nprint(importance_scores['Total'].sort_values(ascending=False).head(top_n))","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:46.782022Z","iopub.status.busy":"2024-12-07T15:47:46.781642Z","iopub.status.idle":"2024-12-07T15:47:47.250436Z","shell.execute_reply":"2024-12-07T15:47:47.247542Z"},"id":"99db2191","outputId":"e4e6e3d7-cd84-4bf0-e127-d84bc067adb5","papermill":{"duration":0.577143,"end_time":"2024-12-07T15:47:47.264436","exception":false,"start_time":"2024-12-07T15:47:46.687293","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# build up pipeline for structured preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MinMaxScaler\n\n# determine categorical and numerical features\nnumerical_feature = df2.select_dtypes(include=['int64', 'float64']).columns\n# categorical_ix = df2.select_dtypes(include=['object', 'bool']).columns\n\n# Preprocessing for numeric and categorical features\nlabel_cols = [\"benign\",\"label\"]\nskip_cols = categorical_features + label_cols\npass_cols = [col for col in df2.columns if col not in skip_cols]\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", categorical_transformer, categorical_features),\n    ],remainder='passthrough')\n\n# Create the pipeline\npipeline = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"variance_filter\", VarianceThreshold(threshold = VAR_THR)),\n    (\"correlation_filter\", CorrelationFilter(threshold=CORR_THR)),\n    (\"scaler\", StandardScaler()),\n    (\"pca\", pca)\n])","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:47.380642Z","iopub.status.busy":"2024-12-07T15:47:47.380283Z","iopub.status.idle":"2024-12-07T15:47:47.39869Z","shell.execute_reply":"2024-12-07T15:47:47.397866Z"},"id":"369bdca8","papermill":{"duration":0.054459,"end_time":"2024-12-07T15:47:47.401288","exception":false,"start_time":"2024-12-07T15:47:47.346829","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Models Selection & Evaluation\n|Model|Type|Advantage|\n|--|--|--|\n|Logistic Regression|Supervised  |Easy to implement, computational efficient|\n|KNN               |Supervised  |widely-used for classification of known number(k) of clusters, can upgrade to outlier robust variant: DBSCAN|\n|Random Forest     |Supervised|Ensemble decision trees, robust to skewness of data|\n|Auto-encoder      |Unsupervised|Deep Learning model for abnormal detection, which only differentiate benign and attack flow|\n\nSupervised models will be trained and evaluated on 0.1 percent data in classifying 2/8/34 traffic category, including Benign and Attacking case.","metadata":{"id":"3e505a37","papermill":{"duration":0.032683,"end_time":"2024-12-07T15:47:47.46955","exception":false,"start_time":"2024-12-07T15:47:47.436867","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def split(df,label = \"label\"):\n\n    # Sorting our dataset into features and target\n    X = df.drop(label, axis = 1)\n    y = df[label]\n\n    # splitting out dataset to train and test\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, shuffle=True, random_state=RANDOM_STATE)\n\n    # scaling our features\n    scaler = StandardScaler()\n    scaled_X_train = scaler.fit_transform(X_train)\n    scaled_X_test = scaler.fit_transform(X_test)\n\n    # return X_train, X_test, y_train, y_test\n    return scaled_X_train, scaled_X_test, y_train, y_test","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:47.536867Z","iopub.status.busy":"2024-12-07T15:47:47.536455Z","iopub.status.idle":"2024-12-07T15:47:47.543054Z","shell.execute_reply":"2024-12-07T15:47:47.54154Z"},"id":"722890ec","jupyter":{"source_hidden":true},"papermill":{"duration":0.043583,"end_time":"2024-12-07T15:47:47.545494","exception":false,"start_time":"2024-12-07T15:47:47.501911","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# creating dataframes to store result metrics\ncolumns = [\"Logistic Regression\", \"KNN\", \"Random Forest\"]\nindex = [\"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\"]\n\nmetrics_2 = pd.DataFrame(index=index, columns=columns)\nmetrics_8 = pd.DataFrame(index=index, columns=columns)\nmetrics_34 = pd.DataFrame(index=index, columns=columns)","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:47.611853Z","iopub.status.busy":"2024-12-07T15:47:47.611443Z","iopub.status.idle":"2024-12-07T15:47:47.619936Z","shell.execute_reply":"2024-12-07T15:47:47.61882Z"},"id":"f376312c","jupyter":{"source_hidden":true},"papermill":{"duration":0.045313,"end_time":"2024-12-07T15:47:47.622407","exception":false,"start_time":"2024-12-07T15:47:47.577094","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# inserts the metrics of the model into the metrics dataframe\ndef insert_metrics(classes, model_name, metrics):\n    if classes == 2:\n        metrics_2.loc['Accuracy':'F1-Score', model_name] = metrics\n    elif classes == 8:\n        metrics_8.loc['Accuracy':'F1-Score', model_name] = metrics\n    else:\n        metrics_34.loc['Accuracy':'F1-Score', model_name] = metrics","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:47.688558Z","iopub.status.busy":"2024-12-07T15:47:47.688078Z","iopub.status.idle":"2024-12-07T15:47:47.694446Z","shell.execute_reply":"2024-12-07T15:47:47.693289Z"},"id":"aa953598","jupyter":{"source_hidden":true},"papermill":{"duration":0.042479,"end_time":"2024-12-07T15:47:47.696559","exception":false,"start_time":"2024-12-07T15:47:47.65408","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# displays the Classification Report and Confusion Matrix\n# inserts the metrics of the model into the metrics dataframe\ndef evaluate(model, X_test, y_test, target_names, classes, model_name):\n    pred = model.predict(X_test)\n\n    # print(pred[:5])\n    # print(f'y_test.type:{type(y_test)}.shape:{y_test.shape} & y_test.type:{type(pred)}.shape:{pred.shape}')\n    accuracy = accuracy_score(y_test, pred)\n    recall = recall_score(y_test, pred, average='macro')\n    precision = precision_score(y_test, pred, average='macro')\n    f1 = f1_score(y_test, pred, average='macro')\n    insert_metrics(classes, model_name, [accuracy, recall, precision, f1])\n\n    print(classification_report(y_test, pred, digits = 3))\n\n    if classes == 2 or classes == 8:\n        plt.figure(figsize = (4, 2), dpi = 300)\n        ConfusionMatrixDisplay(np.round(confusion_matrix(y_test, pred, normalize=\"true\"), 2),\n                               display_labels = target_names).plot()\n        plt.title(model_name)\n        plt.xticks(rotation=30)","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:47.762678Z","iopub.status.busy":"2024-12-07T15:47:47.762061Z","iopub.status.idle":"2024-12-07T15:47:47.770067Z","shell.execute_reply":"2024-12-07T15:47:47.768948Z"},"id":"25e3280a","jupyter":{"source_hidden":true},"papermill":{"duration":0.044409,"end_time":"2024-12-07T15:47:47.772312","exception":false,"start_time":"2024-12-07T15:47:47.727903","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save and load models for testing on different percentage of data\n!mkdir models\ndef pickle_save(model, name):\n    model_filename = 'models/' + name + '.pkl'\n    with open(model_filename, 'wb') as model_file:\n        pickle.dump(model, model_file)\n\n    print(f\"Model saved to {name}\")\n\ndef pickle_load(name):\n    model_filename = 'models/' + name + '.pkl'\n    try:\n        with open(model_filename, 'rb') as model_file:\n            model = pickle.load(model_file)\n        print(f\"Model loaded from {model_filename}\")\n        return model\n    except FileNotFoundError:\n        print(f\"Error: The file '{model_filename}' does not exist.\")\n        return None\n    except pickle.UnpicklingError:\n        print(f\"Error: Failed to load the model from '{model_filename}'. The file might be corrupted.\")\n        return None","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:47.83768Z","iopub.status.busy":"2024-12-07T15:47:47.837289Z","iopub.status.idle":"2024-12-07T15:47:49.037652Z","shell.execute_reply":"2024-12-07T15:47:49.036206Z"},"id":"551b4dc2","jupyter":{"source_hidden":true},"outputId":"0b0162a7-71d4-42db-b338-6a4d497b82b7","papermill":{"duration":1.236432,"end_time":"2024-12-07T15:47:49.040622","exception":false,"start_time":"2024-12-07T15:47:47.80419","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(df2_xs)\ndf_processed = pd.DataFrame(pipeline.fit_transform(df2))\n# df_processed = pd.DataFrame(pca.fit_transform(df_reduced))\n# df_processed = pd.read_csv(\"/kaggle/input/pca-transformed/pca_transformed_data_v11.csv\")\n# print(df_pca.head())\nprint(df_processed.shape)\ndf_processed[\"label\"] = label2\nscaled_X_train, scaled_X_test, y_train, y_test = split(df_processed)\ntarget_names = [\"Attack\", \"Benign\"]\n# print(label2.head())","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:49.107264Z","iopub.status.busy":"2024-12-07T15:47:49.106052Z","iopub.status.idle":"2024-12-07T15:47:50.659248Z","shell.execute_reply":"2024-12-07T15:47:50.658129Z"},"id":"afe6a460","outputId":"d19d6e6c-240c-4923-bd4f-040fe622db65","papermill":{"duration":1.589913,"end_time":"2024-12-07T15:47:50.662039","exception":false,"start_time":"2024-12-07T15:47:49.072126","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logreg_model = LogisticRegression(C = 1, max_iter=500, solver='lbfgs', penalty='l2', random_state=42)\nlogreg_model.fit(scaled_X_train, y_train)\n\nevaluate(logreg_model, scaled_X_test, y_test, target_names, 2, \"Logistic Regression\")\npickle_save(logreg_model, \"logreg_2\")","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:50.732055Z","iopub.status.busy":"2024-12-07T15:47:50.73104Z","iopub.status.idle":"2024-12-07T15:47:51.552811Z","shell.execute_reply":"2024-12-07T15:47:51.551674Z"},"id":"b98340a9","outputId":"8ed4811f-8116-49b9-cdbf-eb4f870adce1","papermill":{"duration":0.859697,"end_time":"2024-12-07T15:47:51.555033","exception":false,"start_time":"2024-12-07T15:47:50.695336","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 8)\nknn.fit(scaled_X_train, y_train)\n\nevaluate(knn, scaled_X_test, y_test, target_names, 2, \"KNN\")\npickle_save(knn, \"knn_2\")","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:51.622776Z","iopub.status.busy":"2024-12-07T15:47:51.62239Z","iopub.status.idle":"2024-12-07T15:47:53.144895Z","shell.execute_reply":"2024-12-07T15:47:53.143722Z"},"id":"ef8a6a0d","outputId":"30773d7c-b486-4219-9a59-1fd18ac3cce3","papermill":{"duration":1.559373,"end_time":"2024-12-07T15:47:53.147271","exception":false,"start_time":"2024-12-07T15:47:51.587898","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Weight-balanced\n# rfc = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\")\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(scaled_X_train, y_train)\n\nevaluate(rfc, scaled_X_test, y_test, target_names, 2, \"Random Forest\")\npickle_save(rfc, \"rfc_2\")","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:47:53.217283Z","iopub.status.busy":"2024-12-07T15:47:53.216877Z","iopub.status.idle":"2024-12-07T15:48:08.269303Z","shell.execute_reply":"2024-12-07T15:48:08.268063Z"},"id":"99ac82a4","outputId":"9a1176fd-e0f8-4f7a-ef97-ee862e3e68e3","papermill":{"duration":15.090253,"end_time":"2024-12-07T15:48:08.271768","exception":false,"start_time":"2024-12-07T15:47:53.181515","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# exporting and displaying the class 2 metrics\nmetrics_2.to_csv(f'{output_path}/metrics_2.csv', index=True)\nmetrics_2","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:48:08.342246Z","iopub.status.busy":"2024-12-07T15:48:08.341623Z","iopub.status.idle":"2024-12-07T15:48:08.356439Z","shell.execute_reply":"2024-12-07T15:48:08.355203Z"},"id":"55e25384","outputId":"d988afcf-f5c1-4f39-8566-b05db87d6bec","papermill":{"duration":0.052624,"end_time":"2024-12-07T15:48:08.358673","exception":false,"start_time":"2024-12-07T15:48:08.306049","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 8 Classes","metadata":{"id":"704c156d","papermill":{"duration":0.034234,"end_time":"2024-12-07T15:48:08.426628","exception":false,"start_time":"2024-12-07T15:48:08.392394","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df_processed = pd.DataFrame(pipeline.fit_transform(df8))\ndf_processed['label']=label8\ntarget_names = [\"Benign\", \"BruteForce\", \"DDoS\", \"Dos\", \"Mirai\", \"Recon\", \"Spoofing\", \"Web\"]\nscaled_X_train, scaled_X_test, y_train, y_test = split(df_processed)","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:48:08.496854Z","iopub.status.busy":"2024-12-07T15:48:08.496458Z","iopub.status.idle":"2024-12-07T15:48:08.571014Z","shell.execute_reply":"2024-12-07T15:48:08.57014Z"},"id":"08b6c99b","papermill":{"duration":0.112815,"end_time":"2024-12-07T15:48:08.573489","exception":false,"start_time":"2024-12-07T15:48:08.460674","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logreg_model = LogisticRegression(C = 1, max_iter=500, solver='lbfgs', penalty='l2', random_state=42)\nlogreg_model.fit(scaled_X_train, y_train)\n\nevaluate(logreg_model, scaled_X_test, y_test, target_names, 8, \"Logistic Regression\")\npickle_save(logreg_model, \"logreg_8\")","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:48:08.642795Z","iopub.status.busy":"2024-12-07T15:48:08.642423Z","iopub.status.idle":"2024-12-07T15:48:15.773318Z","shell.execute_reply":"2024-12-07T15:48:15.772185Z"},"id":"edb0c944","outputId":"32ddae1d-31db-497f-eded-150bd6673afd","papermill":{"duration":7.168727,"end_time":"2024-12-07T15:48:15.775946","exception":false,"start_time":"2024-12-07T15:48:08.607219","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 8)\nknn.fit(scaled_X_train, y_train)\n\nevaluate(knn, scaled_X_test, y_test, target_names, 8, \"KNN\")\npickle_save(knn, \"knn_8\")","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:48:15.847925Z","iopub.status.busy":"2024-12-07T15:48:15.847531Z","iopub.status.idle":"2024-12-07T15:48:18.375795Z","shell.execute_reply":"2024-12-07T15:48:18.374662Z"},"id":"519ca172","outputId":"53888eb2-7256-4c45-b44e-d9dca30020d2","papermill":{"duration":2.566873,"end_time":"2024-12-07T15:48:18.377933","exception":false,"start_time":"2024-12-07T15:48:15.81106","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\")\nrfc.fit(scaled_X_train, y_train)\n\nevaluate(rfc, scaled_X_test, y_test, target_names, 8, \"Random Forest\")\npickle_save(rfc, \"rfc_8\")","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:48:18.452068Z","iopub.status.busy":"2024-12-07T15:48:18.451691Z","iopub.status.idle":"2024-12-07T15:48:35.808973Z","shell.execute_reply":"2024-12-07T15:48:35.80785Z"},"id":"d61b246c","outputId":"c95f51bd-825d-45d9-ba8a-e7ec3ee1cb33","papermill":{"duration":17.396826,"end_time":"2024-12-07T15:48:35.811243","exception":false,"start_time":"2024-12-07T15:48:18.414417","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# exporting and displaying the class 8 metrics\nmetrics_8.to_csv(f'{output_path}/metrics_8.csv', index=True)\nmetrics_8","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:48:35.888313Z","iopub.status.busy":"2024-12-07T15:48:35.887379Z","iopub.status.idle":"2024-12-07T15:48:35.899357Z","shell.execute_reply":"2024-12-07T15:48:35.898224Z"},"id":"89b53437","outputId":"c93d403f-4093-4b94-ea5f-ee02e925ba5c","papermill":{"duration":0.053058,"end_time":"2024-12-07T15:48:35.901449","exception":false,"start_time":"2024-12-07T15:48:35.848391","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 34 Classes","metadata":{"id":"d6976713","papermill":{"duration":0.036871,"end_time":"2024-12-07T15:48:35.975443","exception":false,"start_time":"2024-12-07T15:48:35.938572","status":"completed"},"tags":[]}},{"cell_type":"code","source":"df_processed = pd.DataFrame(pipeline.fit_transform(df34))\ndf_processed['label']=label34\nscaled_X_train, scaled_X_test, y_train, y_test = split(df_processed)\nprint(y_test.value_counts())\ntarget_names = list(y_test.unique())","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:48:36.053529Z","iopub.status.busy":"2024-12-07T15:48:36.05309Z","iopub.status.idle":"2024-12-07T15:48:36.145944Z","shell.execute_reply":"2024-12-07T15:48:36.144756Z"},"id":"e505c31a","outputId":"0493920e-6393-4637-d9c4-ed08ac91ecfc","papermill":{"duration":0.13564,"end_time":"2024-12-07T15:48:36.148381","exception":false,"start_time":"2024-12-07T15:48:36.012741","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logreg_model = LogisticRegression(C = 1, max_iter=500, solver='lbfgs', penalty='l2', random_state=42)\nlogreg_model.fit(scaled_X_train, y_train)\n\nevaluate(logreg_model, scaled_X_test, y_test, target_names, 34, \"Logistic Regression\")\npickle_save(logreg_model, \"logreg_34\")","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:48:36.227762Z","iopub.status.busy":"2024-12-07T15:48:36.227417Z","iopub.status.idle":"2024-12-07T15:49:04.215003Z","shell.execute_reply":"2024-12-07T15:49:04.21373Z"},"id":"f2d65fc9","outputId":"92532fd4-246a-4ecb-f926-5ec08f148c22","papermill":{"duration":28.029733,"end_time":"2024-12-07T15:49:04.217378","exception":false,"start_time":"2024-12-07T15:48:36.187645","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 34)\nknn.fit(scaled_X_train, y_train)\n\nevaluate(knn, scaled_X_test, y_test, target_names, 34, \"KNN\")\npickle_save(knn, \"knn_34\")","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:49:04.298414Z","iopub.status.busy":"2024-12-07T15:49:04.297491Z","iopub.status.idle":"2024-12-07T15:49:06.079031Z","shell.execute_reply":"2024-12-07T15:49:06.077851Z"},"id":"b759237a","outputId":"a980f31b-ce24-4cd2-d3fb-e2c55ae652bd","papermill":{"duration":1.826026,"end_time":"2024-12-07T15:49:06.081592","exception":false,"start_time":"2024-12-07T15:49:04.255566","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\")\nrfc.fit(scaled_X_train, y_train)\n\nevaluate(rfc, scaled_X_test, y_test, target_names, 34, \"Random Forest\")\npickle_save(rfc, \"rfc_34\")","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:49:06.160425Z","iopub.status.busy":"2024-12-07T15:49:06.159049Z","iopub.status.idle":"2024-12-07T15:49:31.461988Z","shell.execute_reply":"2024-12-07T15:49:31.460866Z"},"id":"cbd77802","outputId":"28069bd0-c36f-4ea4-b06f-9f741d19dc51","papermill":{"duration":25.344773,"end_time":"2024-12-07T15:49:31.464109","exception":false,"start_time":"2024-12-07T15:49:06.119336","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# exporting and displaying the class 34 metrics\nmetrics_34.to_csv(f'{output_path}/metrics_34.csv', index=True)\nmetrics_34","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:49:31.542962Z","iopub.status.busy":"2024-12-07T15:49:31.542548Z","iopub.status.idle":"2024-12-07T15:49:31.554919Z","shell.execute_reply":"2024-12-07T15:49:31.553914Z"},"id":"9d578fdf","outputId":"cd2b1cba-3531-4067-c263-339ca197c2ad","papermill":{"duration":0.054979,"end_time":"2024-12-07T15:49:31.557107","exception":false,"start_time":"2024-12-07T15:49:31.502128","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of model names\nmodel_names = [\"Logistic Regression\", \"KNN\", \"Random Forest\"]\n\n# List of dataframes\ndataframes = [metrics_2, metrics_8, metrics_34]\n\n# List of classes\nclasses = [2, 8, 34]\n\n# List of metric names\nmetrics = [\"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\"]\n\n# Create a color mapping dictionary for each model\nmodel_colors = {\n    \"Logistic Regression\": 'tab:blue',\n    \"KNN\": 'tab:red',\n    \"Random Forest\": 'tab:olive',\n}\n\n# Creating subplots for each metric\nfor metric in metrics:\n    fig, axs = plt.subplots(1, len(classes), figsize=(12, 4), sharey=True)\n    fig.suptitle(f\"{metric} Scores\")\n\n    for i, df in enumerate(dataframes):\n        axs[i].set_title(f\"Class {classes[i]}\")\n\n        # Plotting the respective metric score for each model with specified color\n        for model_name in model_names:\n            color = model_colors[model_name]\n            axs[i].bar(model_name, df.loc[metric, model_name], color=color)\n\n        # Set x-axis ticks and labels, rotating labels by 90 degrees\n        axs[i].set_xticks(range(len(model_names)))\n        axs[i].set_xticklabels(model_names, rotation=30)\n\n    # Save the figure\n    plt.savefig(f'{output_path}/{metric}_scores.png', bbox_inches='tight')\n\n    plt.show()","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:49:31.635016Z","iopub.status.busy":"2024-12-07T15:49:31.634659Z","iopub.status.idle":"2024-12-07T15:49:34.346431Z","shell.execute_reply":"2024-12-07T15:49:34.345241Z"},"id":"05fba0d7","outputId":"421de5e4-4989-447d-b2aa-892bf74ad814","papermill":{"duration":2.753675,"end_time":"2024-12-07T15:49:34.34882","exception":false,"start_time":"2024-12-07T15:49:31.595145","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Efficient Intrution Detection\nIn real-world application, it requires that the detection will be effective as well as efficient.\nOne scenario will be analyzed: how models trained in smaller percentage of data perform on larger percentage of data. Though some degree of performance downgrade is observed, the models can stil function and some interesitng findings here:\n\n\n*   Random Forest perform better for high-density data and worse on low-density ones. The reason behind is likely its tendency of overfitting despite of ensembling the average output of decision tree.\n*   KNN perform just the opposite due to its nature to aggregate the local neighbours.\n\n\n","metadata":{"id":"c910d81d","papermill":{"duration":0.040477,"end_time":"2024-12-07T15:49:34.430893","exception":false,"start_time":"2024-12-07T15:49:34.390416","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"## Evaluation of Models Trained on Smaller Data Subsets Applied to Larger Datasets","metadata":{"id":"69f13bcf","papermill":{"duration":0.040978,"end_time":"2024-12-07T15:49:34.512677","exception":false,"start_time":"2024-12-07T15:49:34.471699","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# test model on larger percentage\nlabel8_s = df8_s[\"label\"]\ndf8 = df8_s.drop([\"label\"],axis=1)\n\n# Convert NumPy array to DataFrame\ndf_processed = pd.DataFrame(pipeline.fit_transform(df8))\ndf_processed['label'] = label8_s\nscaled_X_train, scaled_X_test, y_train, y_test = split(df_processed)\ntarget_names = [\"Benign\", \"BruteForce\", \"DDoS\", \"Dos\", \"Mirai\", \"Recon\", \"Spoofing\", \"Web\"]\n\nrfc8_load = pickle_load(\"rfc_8\")\nevaluate(rfc8_load, scaled_X_test , y_test, target_names, 8, \"Random Forest\")\n\nknn8_load = pickle_load(\"knn_8\")\nevaluate(knn8_load, scaled_X_test , y_test, target_names, 8, \"KNN\")","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:49:34.597088Z","iopub.status.busy":"2024-12-07T15:49:34.596714Z","iopub.status.idle":"2024-12-07T15:49:57.540182Z","shell.execute_reply":"2024-12-07T15:49:57.539048Z"},"id":"a6c867de","outputId":"99d11116-96bd-40f0-d6df-e8839f4335bc","papermill":{"duration":22.988897,"end_time":"2024-12-07T15:49:57.542839","exception":false,"start_time":"2024-12-07T15:49:34.553942","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# extract the label value\nlabel8_m = df8_m[\"label\"]\ndf8 = df8_m.drop([\"label\"],axis=1)\n\n# Convert NumPy array to DataFrame\ndf_processed = pd.DataFrame(pipeline.fit_transform(df8))\ndf_processed['label'] = label8_m\nscaled_X_train, scaled_X_test, y_train, y_test = split(df_processed)\ntarget_names = [\"Benign\", \"BruteForce\", \"DDoS\", \"Dos\", \"Mirai\", \"Recon\", \"Spoofing\", \"Web\"]\n\nrfc8_load = pickle_load(\"rfc_8\")\nevaluate(rfc8_load, scaled_X_test , y_test, target_names, 8, \"Random Forest\")\n\nknn8_load = pickle_load(\"knn_8\")\nevaluate(knn8_load, scaled_X_test , y_test, target_names, 8, \"KNN\")","metadata":{"execution":{"iopub.execute_input":"2024-12-07T15:49:57.634594Z","iopub.status.busy":"2024-12-07T15:49:57.634147Z","iopub.status.idle":"2024-12-07T15:50:42.418216Z","shell.execute_reply":"2024-12-07T15:50:42.416933Z"},"id":"613033a3","outputId":"9334e979-a44b-4101-baa4-09c80ff49384","papermill":{"duration":44.832909,"end_time":"2024-12-07T15:50:42.420646","exception":false,"start_time":"2024-12-07T15:49:57.587737","status":"completed"},"tags":[]},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Autoencoder\nIf time permitted, autoencoder will be used to compare the effect of differentiating benign/malicious traffic with classical models.\nInspired by https://www.kaggle.com/code/alexandrelemercier/quantized-autoencoder-qae-ids-for-iot-devices","metadata":{"id":"e00c64ec","papermill":{"duration":0.04464,"end_time":"2024-12-07T15:50:42.512041","exception":false,"start_time":"2024-12-07T15:50:42.467401","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nimport matplotlib.pyplot as plt\nclass AutoencoderAnomalyDetector(nn.Module):\n    def __init__(self, input_dim, threshold=0.1):\n        super(AutoencoderAnomalyDetector, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 16),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(16, 32),\n            nn.ReLU(),\n            nn.Linear(32, 64),\n            nn.ReLU(),\n            nn.Linear(64, 128),\n            nn.ReLU(),\n            nn.Linear(128, input_dim),\n            nn.Sigmoid()\n        )\n        self.threshold = threshold\n\n    def forward(self, x):\n        latent = self.encoder(x)\n        reconstructed = self.decoder(latent)\n        return reconstructed\n\n    def predict(self, x):\n        with torch.no_grad():\n            reconstructed = self.forward(x)\n            reconstruction_error = torch.mean((x - reconstructed) ** 2, dim=1)\n            return (reconstruction_error > self.threshold).int()\n\n    @staticmethod\n    def get_device():\n        return \"cpu\"\n        # return torch.device('mps' if torch.backends.mps.is_available() else\n        #                     'cuda' if torch.cuda.is_available() else 'cpu')\n\n    def train_model(self, dataloader, epochs=20, lr=1e-3):\n        device = self.get_device()\n        self.to(device)\n        optimizer = optim.Adam(self.parameters(), lr=lr)\n        criterion = nn.MSELoss()\n        self.train()\n\n        epoch_losses = []\n        reconstruction_error = []\n        for epoch in range(epochs):\n            total_loss = 0\n            for batch in dataloader:\n                data = batch[0].to(device)\n                optimizer.zero_grad()\n                reconstructed = self.forward(data)\n                loss = criterion(reconstructed, data)\n                loss.backward()\n                optimizer.step()\n                reconstruction_error.extend(torch.mean((data - reconstructed) ** 2, dim=1).cpu().detach().numpy())\n                total_loss += loss.item()\n            avg_loss = total_loss / len(dataloader)\n            epoch_losses.append(avg_loss)\n            # print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n        \n        self.threshold = np.mean(reconstruction_error) + 1.5 * np.std(reconstruction_error)\n        self.plot_learning_curve(epoch_losses)\n        self.plot_reconstruction_error_boxplot(reconstruction_error, self.threshold, \"Reconstruction Error Distribution\")\n        return epoch_losses\n\n    def evaluate(self, dataloader, y_true):\n        device = self.get_device()\n        self.to(device)\n        self.eval()\n        y_pred = []\n        with torch.no_grad():\n            for batch in dataloader:\n                data = batch[0].to(device)\n                preds = self.predict(data)\n                y_pred.extend(preds.cpu().numpy())\n        # print(f'y_pred:{len(y_pred)} & y_true:{len(y_true)}')\n        print(classification_report(y_true, y_pred, digits = 3))\n        plt.figure(figsize = (4, 2), dpi = 300)\n        target_names = [\"Attack\", \"Benign\"]\n        ConfusionMatrixDisplay(np.round(confusion_matrix(y_true, y_pred, normalize=\"true\"), 4),\n                               display_labels = target_names).plot()\n        plt.title(model_name)\n        plt.xticks(rotation=30)\n\n    @staticmethod\n    def plot_learning_curve(losses):\n        plt.figure(figsize=(8, 6))\n        plt.plot(range(1, len(losses) + 1), losses, marker='o', linestyle='-', color='b')\n        plt.title(\"Learning Curve\")\n        plt.xlabel(\"Epoch\")\n        plt.ylabel(\"Loss\")\n        plt.grid(True)\n        plt.show()\n    \n    # Plotting function remains unchanged\n    @staticmethod\n    def plot_reconstruction_error_boxplot(reconstruction_error, threshold, title):\n        plt.figure(figsize=(10, 6))\n        sns.boxplot(x=reconstruction_error, color='blue')\n        plt.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')\n        plt.title(title)\n        plt.xlabel('Reconstruction Error')\n        plt.legend()\n        plt.xlim([0, np.percentile(reconstruction_error, 99)])\n        plt.show()\n\nfrom torch.utils.data import DataLoader, TensorDataset\n\nnormal = df2[label2] # Normal data\nprint(f'normal.shape:{normal.shape}')\n\n# Combine the normal and anomaly data\ndf_tr = pipeline.fit_transform(normal) # Normal data\n\n# print(f'df_tr.shape:{df_tr.shape}')\n# Convert to PyTorch tensors\nX = torch.tensor(df_tr, dtype=torch.float32)\n# y = torch.tensor(y_train.values, dtype=torch.float32)\n\n# DataLoader\ndataset = TensorDataset(X)\ndataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n\n# Train the model\ndetector = AutoencoderAnomalyDetector(input_dim=X.shape[1])\ntrain_losses = detector.train_model(dataloader, epochs=40, lr=1e-3)\n\n# Evaluate the model\ndf_val = pipeline.fit_transform(df2) # Normal data\n\n# Convert to PyTorch tensors\nX_val = torch.tensor(df_val, dtype=torch.float32)\ny_val = torch.tensor(label2.values, dtype=torch.float32)\n\n# DataLoader\ndataset = TensorDataset(X_val)\ntest_loader = DataLoader(dataset, batch_size=128, shuffle=True)\ndetector.evaluate(test_loader, y_val)\n","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Potential integration with IPS(Intrusion Prevention System)\nIn Kaggle community, there are existing feature importance analysis based on different model. Meanwhile, there exists many fine-tuning framework in the market, such as Optuna, which can find the best parameter for identifying attack traffic in accuracy. Combined with these efforts, it is promising an automatic threshold setting   on configurable parameter of IPS, such as Suricata, will be able to adaptively block the intrusion attempt.\n\n*This can be good research topic for future.*","metadata":{"id":"3f6ec182","papermill":{"duration":0.045451,"end_time":"2024-12-07T15:51:10.000535","exception":false,"start_time":"2024-12-07T15:51:09.955084","status":"completed"},"tags":[]}}]}