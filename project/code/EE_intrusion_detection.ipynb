{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6425404,"sourceType":"datasetVersion","datasetId":3706998},{"sourceId":10115500,"sourceType":"datasetVersion","datasetId":6241086},{"sourceId":147704369,"sourceType":"kernelVersion"},{"sourceId":209881150,"sourceType":"kernelVersion"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Table of Contents\nThe \n* Effective Machine Learning\n    * Disproportional Data sampling\n    * EDA & Preprocessing\n    * Model Selection and Evaluation\n* Efficient Machine Learning \n    * Model performance on different scale of sampling data\n    * Potential integration with IPS(Intrusion Prevention System)\n","metadata":{}},{"cell_type":"markdown","source":"# Effective Machine Learning\nBelow is a summary of the performance metrics for each model tested on the **validation** dataset:\n","metadata":{}},{"cell_type":"code","source":"import sklearn as sk\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay, accuracy_score, recall_score, precision_score, f1_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.cluster import KMeans\n\nimport pickle\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport sys\nfrom PIL import Image\nimport warnings\nfrom tqdm import tqdm, trange\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nRANDOM_STATE = 42\n# for lib in [pd, sk, sns, sv, optuna, tf, keras]:\nfor lib in [pd, sk, sns]:\n    print(f\"Using {lib.__name__} {lib.__version__}\")\n\n# define the thresthold for proprocessors\nVAR_THR = 0.03\nCORR_THR = 0.9\n","metadata":{"ExecuteTime":{"end_time":"2024-08-21T11:48:20.817741Z","start_time":"2024-08-21T11:47:39.157249Z"},"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:12:47.677552Z","iopub.execute_input":"2024-12-06T19:12:47.678376Z","iopub.status.idle":"2024-12-06T19:12:47.700694Z","shell.execute_reply.started":"2024-12-06T19:12:47.678333Z","shell.execute_reply":"2024-12-06T19:12:47.699443Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Importing Data\nSince the original dataset is extremely bias so a dispropotional sampling was applied.","metadata":{}},{"cell_type":"code","source":"# 0.1 percent as size xs\ndf2_xs = pd.read_csv(\"/kaggle/input/disproportionate-sampling-dataset-for-ciciot2023/0.001_dist_percent_2classes.csv\")\ndf8_xs = pd.read_csv(\"/kaggle/input/disproportionate-sampling-dataset-for-ciciot2023/0.001_dist_percent_8classes.csv\")\ndf34_xs = pd.read_csv(\"/kaggle/input/disproportionate-sampling-dataset-for-ciciot2023/0.001_dist_percent_34classes.csv\")\n\n# 0.5 percent as size s\ndf2_s = pd.read_csv(\"/kaggle/input/disproportionate-sampling-dataset-for-ciciot2023/0.005_dist_percent_2classes.csv\")\ndf8_s = pd.read_csv(\"/kaggle/input/disproportionate-sampling-dataset-for-ciciot2023/0.005_dist_percent_8classes.csv\")\ndf34_s = pd.read_csv(\"/kaggle/input/disproportionate-sampling-dataset-for-ciciot2023/0.005_dist_percent_34classes.csv\")\n\n# 1 percent as size m\n# df2_m = pd.read_csv(\"/kaggle/input/disproportionate-sampling-dataset-for-ciciot2023/0.01_dist_percent_2classes.csv\")\ndf8_m = pd.read_csv(\"/kaggle/input/disproportionate-sampling-dataset-for-ciciot2023/0.01_dist_percent_8classes.csv\")\n# df34_m = pd.read_csv(\"/kaggle/input/disproportionate-sampling-dataset-for-ciciot2023/0.01_dist_percent_34classes.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:12:47.702121Z","iopub.execute_input":"2024-12-06T19:12:47.702507Z","iopub.status.idle":"2024-12-06T19:12:53.995270Z","shell.execute_reply.started":"2024-12-06T19:12:47.702478Z","shell.execute_reply":"2024-12-06T19:12:53.994083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# setting the default dataset\ndf2 = df2_xs\ndf8 = df8_xs\ndf34 = df34_xs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:12:53.996733Z","iopub.execute_input":"2024-12-06T19:12:53.997094Z","iopub.status.idle":"2024-12-06T19:12:54.004273Z","shell.execute_reply.started":"2024-12-06T19:12:53.997064Z","shell.execute_reply":"2024-12-06T19:12:54.002950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# extract the label value\nlabel2=df2[\"benign\"]\ndf2 = df2.drop(\"benign\", axis=1)\nlabel8=df8[\"label\"]\ndf8 = df8.drop(\"label\", axis=1)\nlabel34=df34[\"label\"]\ndf34 = df34.drop(\"label\", axis=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:12:54.005951Z","iopub.execute_input":"2024-12-06T19:12:54.006368Z","iopub.status.idle":"2024-12-06T19:12:54.039565Z","shell.execute_reply.started":"2024-12-06T19:12:54.006333Z","shell.execute_reply":"2024-12-06T19:12:54.038081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate percentages for each unique value\nvalue_counts = df2['protocol_type'].value_counts(normalize=True) * 100\n\n# Sort percentages from highest to lowest\nsorted_percentages = value_counts.sort_values(ascending=False)\n\n# Get top 10 percentages (or fewer if there are less than unique values)\ntop_percentages = sorted_percentages.head(10)\n\n# Plot histogram for the top values sorted by percentage\nplt.bar(top_percentages.index, top_percentages.values)\nplt.title(\"Top Percentages Distribution (Highest to Lowest)\")\nplt.xlabel(\"protocol_type\")\nplt.ylabel(\"Percentage (%)\")\nplt.xticks(top_percentages.index)  # Ensure x-axis labels show only top values\nplt.show()\n\n# Print the top sorted percentages\nprint(\"Top 20 Percentages (from Highest to Lowest):\")\nprint(sorted_percentages[:20])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:12:54.040825Z","iopub.execute_input":"2024-12-06T19:12:54.041302Z","iopub.status.idle":"2024-12-06T19:12:54.305284Z","shell.execute_reply.started":"2024-12-06T19:12:54.041270Z","shell.execute_reply":"2024-12-06T19:12:54.304151Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: OneHotEncoding for categorical features\ncategorical_features = ['protocol_type']\ncategorical_transformer = OneHotEncoder(sparse_output=False)\nencoded_categorical = categorical_transformer.fit_transform(df2[categorical_features])\n# print(encoded_categorical)\n\n# Create a DataFrame for the encoded categorical features\nencoded_df = pd.DataFrame(encoded_categorical, columns=categorical_transformer.get_feature_names_out(categorical_features))\n\n# Drop the original categorical columns and concatenate the encoded features\ndf2_filtered = df2.drop(categorical_features, axis=1)\ndf2_filtered = pd.concat([df2_filtered, encoded_df], axis=1)\ndf2_filtered.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:12:54.306841Z","iopub.execute_input":"2024-12-06T19:12:54.307271Z","iopub.status.idle":"2024-12-06T19:12:54.372313Z","shell.execute_reply.started":"2024-12-06T19:12:54.307232Z","shell.execute_reply":"2024-12-06T19:12:54.371106Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"onehot_cols = ['http','https','dns','telnet','smtp','ssh','irc','tcp','udp','dhcp','arp','icmp','ipv','llc']\nonehot_cols.extend(encoded_df.columns.tolist())\n# print(f'onehot_cols:{onehot_cols}')\nfrom scipy.stats import chi2_contingency\n\ndef cramers_v(contingency_table):\n    \"\"\"Calculate Cramér's V for a contingency table.\"\"\"\n    chi2, _, _, _ = chi2_contingency(contingency_table)\n    n = contingency_table.sum().sum()\n    return np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n\ndef filter_strong_association_features(df, target, threshold=0.3):\n    \"\"\"\n    Filter binary/categorical features strongly associated with a categorical target label.\n    \n    Parameters:\n        df (pd.DataFrame): DataFrame containing features and the target label.\n        target_col (str): Name of the categorical target label column.\n        threshold (float): Minimum Cramér's V value to consider a strong association.\n    \n    Returns:\n        list: List of feature names with strong association.\n    \"\"\"\n    strong_features = []\n\n    # Loop through features in the DataFrame\n    for col in df.columns:\n        # if col == target_col:  # Skip the target column\n        #     continue\n        if df[col].nunique() <= 10:  # Check if the column is categorical/binary\n            # Create a contingency table\n            contingency_table = pd.crosstab(df[col], target)\n            \n            # Calculate Cramér's V\n            cramer_v_value = cramers_v(contingency_table)\n            \n            # If Cramér's V exceeds the threshold, consider it strongly associated\n            if cramer_v_value >= threshold:\n                strong_features.append(col)\n    \n    return strong_features\n\ncols_valueable = filter_strong_association_features(df2_filtered,label34, threshold=0.3)\nprint(f'cols_valueable in onehotcode:{cols_valueable}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:12:54.374169Z","iopub.execute_input":"2024-12-06T19:12:54.374625Z","iopub.status.idle":"2024-12-06T19:12:55.282244Z","shell.execute_reply.started":"2024-12-06T19:12:54.374579Z","shell.execute_reply":"2024-12-06T19:12:55.280941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# filter out columns with uniform distribution(always the same value)\nskewness = df2_filtered.skew()\n# print(skewness)\n\ndf2_ls = df2_filtered.loc[:,skewness<0.00001]\n\n# Create a grid of subplots\nfig, axes = plt.subplots(3, 3, figsize=(9, 9))  # 3 rows, 3 columns\naxes = axes.flatten()  # Flatten the axes array for easy indexing\n\n# Plot each column's histogram in the corresponding subplot\nfor i, column in enumerate(df2_ls.columns):\n    # Count the occurrences of True and False\n    value_counts = df2_ls[column].value_counts()\n    # Plot the histogram\n    value_counts.plot(kind='bar', color=['skyblue', 'orange'], ax=axes[i])\n    axes[i].set_title(f'{column}')\n    axes[i].set_xlabel('Value')\n    axes[i].set_ylabel('Count')\n    axes[i].set_xticks([0, 1])\n    axes[i].set_xticklabels(['False', 'True'], rotation=0)\n\n# Adjust layout for better spacing\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:12:55.283735Z","iopub.execute_input":"2024-12-06T19:12:55.284235Z","iopub.status.idle":"2024-12-06T19:12:56.683340Z","shell.execute_reply.started":"2024-12-06T19:12:55.284196Z","shell.execute_reply":"2024-12-06T19:12:56.682150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# analyze the colunmn with high abs skew values\ndf2_hs = df2_filtered.loc[:,abs(skewness)>50]\n\n# print(df2_hs.head(10))\n\n# # Identify continuous columns (e.g., numeric columns)\ncontinuous_columns = df2_hs.select_dtypes(include=['float64', 'int64']).columns\n\n# Set up the plotting\nplt.figure(figsize=(10, 10))\n\n# delete columns with too high skewness\nfrom scipy.stats import mstats\n\ndf_winsorized = df2_hs[continuous_columns].apply(lambda x: mstats.winsorize(x, limits=[0.05, 0.05]))\n\n# Loop through continuous columns and plot histograms\nfor i, col in enumerate(continuous_columns, 1):\n    plt.subplot(5,5, i)  \n    sns.histplot(df_winsorized[col], kde=True, bins=10, color='skyblue', edgecolor='black')  # histogram with KDE\n    plt.title(f'{col}')\n    plt.xlabel(col)\n    plt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:12:56.685272Z","iopub.execute_input":"2024-12-06T19:12:56.685675Z","iopub.status.idle":"2024-12-06T19:13:00.489251Z","shell.execute_reply.started":"2024-12-06T19:12:56.685639Z","shell.execute_reply":"2024-12-06T19:13:00.488068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 2: Variance threshold for low variance\nfrom sklearn.feature_selection import VarianceThreshold\nvar_thr = VarianceThreshold(threshold = VAR_THR) #Removing both constant and quasi-constant\nvar_thr.fit(df2_filtered)\n\nvar_thr.get_support()\nconcol = [column for column in df2_filtered.columns \n          if column not in df2_filtered.columns[var_thr.get_support()]]\nprint(f'columns of low variance: {concol}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:00.490506Z","iopub.execute_input":"2024-12-06T19:13:00.490809Z","iopub.status.idle":"2024-12-06T19:13:00.550234Z","shell.execute_reply.started":"2024-12-06T19:13:00.490783Z","shell.execute_reply":"2024-12-06T19:13:00.549081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# remove the feature of both low variance and low \ncols_to_drop = [col for col in concol if col not in cols_valueable]\nprint(f'cols_to_drop: {cols_to_drop}')\ndf2_normal = df2_filtered.drop(concol,axis =1)\ndf2_normal.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:00.551728Z","iopub.execute_input":"2024-12-06T19:13:00.552141Z","iopub.status.idle":"2024-12-06T19:13:00.586060Z","shell.execute_reply.started":"2024-12-06T19:13:00.552098Z","shell.execute_reply":"2024-12-06T19:13:00.584719Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show boxplot of parameter with comparatively normal distribution\ncontinuous_columns = df2_normal.select_dtypes(include=['float64', 'int64']).columns\n\n# Create a grid for the boxplots\nfig, axes = plt.subplots(6, 5, figsize=(12, 12))\n\n# Flatten the axes array for easier iteration\naxes = axes.flatten()\n\n# Loop through each column and plot the boxplot\nfor i, col in enumerate(continuous_columns):\n    sns.boxplot(x=df2_normal[col], ax=axes[i])\n    axes[i].set_title(f'{col}')\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n# Show the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:00.587571Z","iopub.execute_input":"2024-12-06T19:13:00.588054Z","iopub.status.idle":"2024-12-06T19:13:04.711497Z","shell.execute_reply.started":"2024-12-06T19:13:00.588016Z","shell.execute_reply":"2024-12-06T19:13:04.710327Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Correlation Analysis\ndef show_corr(df):\n    corr_matrix = df.corr()\n    \n    # Visualize the correlation matrix\n    \n    plt.figure(figsize=(20, 20))\n    \n    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n    plt.xticks(rotation=90)\n    plt.title('Correlation Matrix')\n    plt.show()\nshow_corr(df2_normal)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:04.712816Z","iopub.execute_input":"2024-12-06T19:13:04.713245Z","iopub.status.idle":"2024-12-06T19:13:08.346698Z","shell.execute_reply.started":"2024-12-06T19:13:04.713210Z","shell.execute_reply":"2024-12-06T19:13:08.345098Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 3: Correlation filtering (custom function)\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass CorrelationFilter(BaseEstimator, TransformerMixin):\n    def __init__(self, threshold=0.9):\n        self.threshold = threshold\n        self.to_drop_ = None\n\n    def fit(self, X, y=None):\n        # Ensure X is a DataFrame\n        X = pd.DataFrame(X)\n        \n        # Compute the correlation matrix\n        corr_matrix = X.corr().abs()\n        \n        # Identify highly correlated features\n        upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n        self.to_drop_ = [column for column in upper_triangle.columns if any(upper_triangle[column] > self.threshold)]\n        print(f'column of high correlation({self.threshold}) to be dropped: {self.to_drop_}')\n        return self\n\n    def transform(self, X):\n        # Ensure X is a DataFrame\n        X = pd.DataFrame(X)\n        \n        # Drop identified columns\n        return X.drop(columns=self.to_drop_, errors='ignore')\n\n    def fit_transform(self, X, y=None):\n        self.fit(X, y)\n        return self.transform(X)\ndf_reduced = CorrelationFilter(threshold=CORR_THR).fit_transform(df2_normal)\n# df_reduced = CorrelationFilter(threshold=0.9).fit_transform(df2_normal)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:08.348015Z","iopub.execute_input":"2024-12-06T19:13:08.348343Z","iopub.status.idle":"2024-12-06T19:13:08.557493Z","shell.execute_reply.started":"2024-12-06T19:13:08.348314Z","shell.execute_reply":"2024-12-06T19:13:08.556066Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dimension Reduction Using PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ndef CumuPlot(df):\n    # Apply PCA with the adjusted number of components\n    pca = PCA()\n    \n    df_pca = pca.fit_transform(df)\n    \n    # Calculate cumulative variance\n    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n    \n    # Plot cumulative explained variance\n    plt.figure(figsize=(10, 8))\n    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-', color='b')\n    plt.axhline(y=0.90, color='m', linestyle='-.', label='90% Threshold')  # Optional threshold line\n    plt.axhline(y=0.95, color='r', linestyle='--', label='95% Threshold')  # Optional threshold line\n    plt.xticks(range(1, len(cumulative_variance) + 1))  # Ensure x-ticks align with components\n    plt.xlabel('Number of Principal Components')\n    plt.ylabel('Cumulative Explained Variance')\n    plt.title('Cumulative Explained Variance by PCA Components')\n    plt.legend()\n    plt.grid()\n    plt.show()\n\nscaler = StandardScaler()\ndf_scaled = scaler.fit_transform(df_reduced)\nCumuPlot(df_scaled)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:08.559077Z","iopub.execute_input":"2024-12-06T19:13:08.559514Z","iopub.status.idle":"2024-12-06T19:13:09.162529Z","shell.execute_reply.started":"2024-12-06T19:13:08.559474Z","shell.execute_reply":"2024-12-06T19:13:09.161414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply PCA and retain the top n components\nn_components = 20 #take 95% variance as target\npca = PCA(n_components=n_components)\n\ndf_scaled = pd.DataFrame(df_scaled)\nprincipal_components = pca.fit_transform(df_scaled)\n\n# print(scaler.fit_transform(principal_components)[:5])\n\n# Get PCA components (loadings)\ncomponents = pca.components_\n\ndf = df_reduced\n# Compute the importance of each feature\nimportance = components.T  # Transpose to make features the rows\nimportance_scores = pd.DataFrame(importance, columns=[f'PC{i+1}' for i in range(components.shape[0])], index=df.columns)\n\n# Calculate the total contribution of each feature across all PCs\ntop_n = 20  # Change this to the desired number of features\nimportance_scores['Total'] = (importance_scores.abs() * pca.explained_variance_ratio_).sum(axis=1)\nprint(f\"Top({top_n}) features of  importance: \")\nprint(importance_scores['Total'].sort_values(ascending=False).head(top_n))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:09.163868Z","iopub.execute_input":"2024-12-06T19:13:09.164205Z","iopub.status.idle":"2024-12-06T19:13:09.547438Z","shell.execute_reply.started":"2024-12-06T19:13:09.164177Z","shell.execute_reply":"2024-12-06T19:13:09.546213Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Models Selection & Evaluation\n|Model|Type|Advantage|\n|--|--|--|\n|Logistic Regression|Supervised  |Easy to implement, computational efficient|\n|KNN               |Supervised  |widely-used for classification of known number(k) of clusters, can upgrade to outlier robust variant: DBSCAN|\n|Random Forest     |Supervised|Ensemble decision trees, robust to skewness of data|\n|Auto-encoder      |Unsupervised|Deep Learning model for abnormal detection, which only differentiate benign and attack flow|\n","metadata":{}},{"cell_type":"markdown","source":"## EDA & Preprocessing\n**Summary Table of EDA Methods and Preprocessing**\nFollowing table is a brief summary of common Exploratory Data Analysis (EDA) and corresponding Preprocessing methods. \n\n| EDA Method|Typical Preprocessing/Feature Engineering|Relevancy to current probelm|\n|--|--|--|\n|Data Cleaning|Handle missing values, duplicates, and outliers.|outlier is the challenge of the dataset, capping techniques are applied to |\n|Univariate Analysis|Normalize/encode features, transform skewed data.|show skewness of the data and visualize with histogram, boxplot, drop column with limited info|\n|Multivariate Analysis|Normalize, reduce multicollinearity, apply dimensionality reduction.|show Correlation table and reduce dimension with PCA/t-CNS|\n|Target Variable Analysis|Balance classes, transform skewed distributions.|the label class is highly biased , espcially for (D)DOS. Dispropotional Sampling is used to balance class |\n|Time Series Analysis|Interpolate missing data, extract date/time features, apply smoothing.|N/A, since the dataset is already engineered and temporal information has been compacted|\n\n","metadata":{}},{"cell_type":"code","source":"def split(df,label = \"label\"):\n    \n    # Sorting our dataset into features and target\n    X = df.drop(label, axis = 1)\n    y = df[label]\n    \n    # splitting out dataset to train and test\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n    \n    # scaling our features\n    scaler = StandardScaler()\n    scaled_X_train = scaler.fit_transform(X_train)\n    scaled_X_test = scaler.fit_transform(X_test)\n    \n    # return X_train, X_test, y_train, y_test\n    return scaled_X_train, scaled_X_test, y_train, y_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:09.548765Z","iopub.execute_input":"2024-12-06T19:13:09.549251Z","iopub.status.idle":"2024-12-06T19:13:09.565979Z","shell.execute_reply.started":"2024-12-06T19:13:09.549211Z","shell.execute_reply":"2024-12-06T19:13:09.563184Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# creating dataframes to store result metrics\ncolumns = [\"Logistic Regression\", \"KNN\", \"Random Forest\"]\nindex = [\"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\"]\n\nmetrics_2 = pd.DataFrame(index=index, columns=columns)\nmetrics_8 = pd.DataFrame(index=index, columns=columns)\nmetrics_34 = pd.DataFrame(index=index, columns=columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:09.570636Z","iopub.execute_input":"2024-12-06T19:13:09.571167Z","iopub.status.idle":"2024-12-06T19:13:09.596732Z","shell.execute_reply.started":"2024-12-06T19:13:09.571124Z","shell.execute_reply":"2024-12-06T19:13:09.594641Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# inserts the metrics of the model into the metrics dataframe\ndef insert_metrics(classes, model_name, metrics):\n    if classes == 2:\n        metrics_2.loc['Accuracy':'F1-Score', model_name] = metrics\n    elif classes == 8:\n        metrics_8.loc['Accuracy':'F1-Score', model_name] = metrics\n    else:\n        metrics_34.loc['Accuracy':'F1-Score', model_name] = metrics","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:09.601776Z","iopub.execute_input":"2024-12-06T19:13:09.602739Z","iopub.status.idle":"2024-12-06T19:13:09.620472Z","shell.execute_reply.started":"2024-12-06T19:13:09.602690Z","shell.execute_reply":"2024-12-06T19:13:09.616448Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# displays the Classification Report and Confusion Matrix\n# inserts the metrics of the model into the metrics dataframe\ndef evaluate(model, X_test, y_test, target_names, classes, model_name):\n    pred = model.predict(X_test)\n\n    # print(pred[:5])\n    # print(f'y_test.type:{type(y_test)}.shape:{y_test.shape} & y_test.type:{type(pred)}.shape:{pred.shape}')\n    accuracy = accuracy_score(y_test, pred)\n    recall = recall_score(y_test, pred, average='macro')\n    precision = precision_score(y_test, pred, average='macro')\n    f1 = f1_score(y_test, pred, average='macro')\n    insert_metrics(classes, model_name, [accuracy, recall, precision, f1])\n    \n    print(classification_report(y_test, pred, digits = 3))\n\n    if classes == 2 or classes == 8:\n        plt.figure(figsize = (4, 2), dpi = 300)\n        ConfusionMatrixDisplay(np.round(confusion_matrix(y_test, pred, normalize=\"true\"), 2), \n                               display_labels = target_names).plot()\n        plt.title(model_name)\n        plt.xticks(rotation=30);","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:09.622420Z","iopub.execute_input":"2024-12-06T19:13:09.624822Z","iopub.status.idle":"2024-12-06T19:13:09.641428Z","shell.execute_reply.started":"2024-12-06T19:13:09.624762Z","shell.execute_reply":"2024-12-06T19:13:09.640587Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# save and load models for testing on different percentage of data\n!mkdir Models\ndef pickle_save(model, name):\n    model_filename = 'Models/' + name + '.pkl'\n    with open(model_filename, 'wb') as model_file:\n        pickle.dump(model, model_file)\n\n    print(f\"Model saved to {name}\")\n\ndef pickle_load(name):\n    model_filename = 'Models/' + name + '.pkl'\n    try:\n        with open(model_filename, 'rb') as model_file:\n            model = pickle.load(model_file)\n        print(f\"Model loaded from {model_filename}\")\n        return model\n    except FileNotFoundError:\n        print(f\"Error: The file '{model_filename}' does not exist.\")\n        return None\n    except pickle.UnpicklingError:\n        print(f\"Error: Failed to load the model from '{model_filename}'. The file might be corrupted.\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:09.642968Z","iopub.execute_input":"2024-12-06T19:13:09.643355Z","iopub.status.idle":"2024-12-06T19:13:10.762433Z","shell.execute_reply.started":"2024-12-06T19:13:09.643326Z","shell.execute_reply":"2024-12-06T19:13:10.760900Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# build up pipeline for structured preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MinMaxScaler\n\n# determine categorical and numerical features\nnumerical_feature = df2.select_dtypes(include=['int64', 'float64']).columns\n# categorical_ix = df2.select_dtypes(include=['object', 'bool']).columns\n\n# Preprocessing for numeric and categorical features\nlabel_cols = [\"benign\",\"label\"]\nskip_cols = categorical_features + label_cols\npass_cols = [col for col in df2.columns if col not in skip_cols]\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", categorical_transformer, categorical_features),\n    ],remainder='passthrough')\n\n# Create the pipeline \npipeline = Pipeline(steps=[\n    (\"preprocessor\", preprocessor),\n    (\"variance_filter\", VarianceThreshold(threshold = VAR_THR)),\n    (\"correlation_filter\", CorrelationFilter(threshold=CORR_THR)),\n    (\"scaler\", StandardScaler()),\n    (\"pca\", pca)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:10.764556Z","iopub.execute_input":"2024-12-06T19:13:10.764977Z","iopub.status.idle":"2024-12-06T19:13:10.778591Z","shell.execute_reply.started":"2024-12-06T19:13:10.764938Z","shell.execute_reply":"2024-12-06T19:13:10.777243Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# print(df2_xs)\ndf_processed = pd.DataFrame(pipeline.fit_transform(df2))\n# df_processed = pd.DataFrame(pca.fit_transform(df_reduced))\n# df_processed = pd.read_csv(\"/kaggle/input/pca-transformed/pca_transformed_data_v11.csv\")\n# print(df_pca.head())\nprint(df_processed.shape)\ndf_processed[\"label\"] = label2\nscaled_X_train, scaled_X_test, y_train, y_test = split(df_processed)\ntarget_names = [\"Attack\", \"Benign\"]\n# print(label2.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:10.779908Z","iopub.execute_input":"2024-12-06T19:13:10.780286Z","iopub.status.idle":"2024-12-06T19:13:12.281767Z","shell.execute_reply.started":"2024-12-06T19:13:10.780255Z","shell.execute_reply":"2024-12-06T19:13:12.280476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logreg_model = LogisticRegression(C = 1, max_iter=500, solver='lbfgs', penalty='l2', random_state=42)\nlogreg_model.fit(scaled_X_train, y_train)\n\nevaluate(logreg_model, scaled_X_test, y_test, target_names, 2, \"Logistic Regression\")\npickle_save(logreg_model, \"logreg_2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:12.283821Z","iopub.execute_input":"2024-12-06T19:13:12.284274Z","iopub.status.idle":"2024-12-06T19:13:12.905070Z","shell.execute_reply.started":"2024-12-06T19:13:12.284234Z","shell.execute_reply":"2024-12-06T19:13:12.903835Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 8)\nknn.fit(scaled_X_train, y_train)\n\nevaluate(knn, scaled_X_test, y_test, target_names, 2, \"KNN\")\npickle_save(knn, \"knn_2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:12.906644Z","iopub.execute_input":"2024-12-06T19:13:12.907016Z","iopub.status.idle":"2024-12-06T19:13:14.278376Z","shell.execute_reply.started":"2024-12-06T19:13:12.906959Z","shell.execute_reply":"2024-12-06T19:13:14.277155Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Weight-balanced\n# rfc = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\")\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(scaled_X_train, y_train)\n\nevaluate(rfc, scaled_X_test, y_test, target_names, 2, \"Random Forest\")\npickle_save(rfc, \"rfc_2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:14.279691Z","iopub.execute_input":"2024-12-06T19:13:14.280073Z","iopub.status.idle":"2024-12-06T19:13:29.713569Z","shell.execute_reply.started":"2024-12-06T19:13:14.280039Z","shell.execute_reply":"2024-12-06T19:13:29.712418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# exporting and displaying the class 2 metrics\nmetrics_2.to_csv('metrics_2.csv', index=True)\nmetrics_2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:29.715357Z","iopub.execute_input":"2024-12-06T19:13:29.715696Z","iopub.status.idle":"2024-12-06T19:13:29.730790Z","shell.execute_reply.started":"2024-12-06T19:13:29.715659Z","shell.execute_reply":"2024-12-06T19:13:29.729609Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 8 Classes","metadata":{}},{"cell_type":"code","source":"# df8_feat = pd.read_csv('/kaggle/working/pca_transformed_data.csv')\n# df_processed = pd.DataFrame(pca.fit_transform(df_reduced))\n# df_processed = pd.DataFrame(pipeline.fit_transform(df8))\ndf_processed['label']=label8\ntarget_names = [\"Benign\", \"BruteForce\", \"DDoS\", \"Dos\", \"Mirai\", \"Recon\", \"Spoofing\", \"Web\"]\nscaled_X_train, scaled_X_test, y_train, y_test = split(df_processed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:29.732261Z","iopub.execute_input":"2024-12-06T19:13:29.732592Z","iopub.status.idle":"2024-12-06T19:13:29.822870Z","shell.execute_reply.started":"2024-12-06T19:13:29.732563Z","shell.execute_reply":"2024-12-06T19:13:29.821756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logreg_model = LogisticRegression(C = 1, max_iter=500, solver='lbfgs', penalty='l2', random_state=42)\nlogreg_model.fit(scaled_X_train, y_train)\n\nevaluate(logreg_model, scaled_X_test, y_test, target_names, 8, \"Logistic Regression\")\npickle_save(logreg_model, \"logreg_8\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:29.824148Z","iopub.execute_input":"2024-12-06T19:13:29.824451Z","iopub.status.idle":"2024-12-06T19:13:35.899364Z","shell.execute_reply.started":"2024-12-06T19:13:29.824426Z","shell.execute_reply":"2024-12-06T19:13:35.898083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 8)\nknn.fit(scaled_X_train, y_train)\n\nevaluate(knn, scaled_X_test, y_test, target_names, 8, \"KNN\")\npickle_save(knn, \"knn_8\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:35.901246Z","iopub.execute_input":"2024-12-06T19:13:35.901688Z","iopub.status.idle":"2024-12-06T19:13:38.164414Z","shell.execute_reply.started":"2024-12-06T19:13:35.901650Z","shell.execute_reply":"2024-12-06T19:13:38.163178Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\")\nrfc.fit(scaled_X_train, y_train)\n\nevaluate(rfc, scaled_X_test, y_test, target_names, 8, \"Random Forest\")\npickle_save(rfc, \"rfc_8\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:38.166050Z","iopub.execute_input":"2024-12-06T19:13:38.166476Z","iopub.status.idle":"2024-12-06T19:13:55.649609Z","shell.execute_reply.started":"2024-12-06T19:13:38.166437Z","shell.execute_reply":"2024-12-06T19:13:55.648236Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# exporting and displaying the class 8 metrics\nmetrics_8.to_csv('metrics_8.csv', index=True)\nmetrics_8","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:55.651273Z","iopub.execute_input":"2024-12-06T19:13:55.651699Z","iopub.status.idle":"2024-12-06T19:13:55.666557Z","shell.execute_reply.started":"2024-12-06T19:13:55.651660Z","shell.execute_reply":"2024-12-06T19:13:55.665197Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 34 Classes","metadata":{}},{"cell_type":"code","source":"# df34_feat = pd.read_csv('/kaggle/working/pca_transformed_data.csv')\n# df34_feat['label']=df34['label']\n# df_processed = pd.DataFrame(pipeline.fit_transform(df34))\ndf_processed['label']=label34\nscaled_X_train, scaled_X_test, y_train, y_test = split(df_processed)\nprint(y_test.value_counts())\ntarget_names = list(y_test.unique())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:55.668312Z","iopub.execute_input":"2024-12-06T19:13:55.668640Z","iopub.status.idle":"2024-12-06T19:13:55.765256Z","shell.execute_reply.started":"2024-12-06T19:13:55.668614Z","shell.execute_reply":"2024-12-06T19:13:55.763918Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"logreg_model = LogisticRegression(C = 1, max_iter=500, solver='lbfgs', penalty='l2', random_state=42)\nlogreg_model.fit(scaled_X_train, y_train)\n\nevaluate(logreg_model, scaled_X_test, y_test, target_names, 34, \"Logistic Regression\")\npickle_save(logreg_model, \"logreg_34\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:13:55.766554Z","iopub.execute_input":"2024-12-06T19:13:55.766860Z","iopub.status.idle":"2024-12-06T19:14:17.905193Z","shell.execute_reply.started":"2024-12-06T19:13:55.766834Z","shell.execute_reply":"2024-12-06T19:14:17.903973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"knn = KNeighborsClassifier(n_neighbors = 34)\nknn.fit(scaled_X_train, y_train)\n\nevaluate(knn, scaled_X_test, y_test, target_names, 34, \"KNN\")\npickle_save(knn, \"knn_34\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:14:17.906476Z","iopub.execute_input":"2024-12-06T19:14:17.906783Z","iopub.status.idle":"2024-12-06T19:14:19.622545Z","shell.execute_reply.started":"2024-12-06T19:14:17.906755Z","shell.execute_reply":"2024-12-06T19:14:19.621227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rfc = RandomForestClassifier(n_estimators=100, class_weight=\"balanced\")\nrfc.fit(scaled_X_train, y_train)\n\nevaluate(rfc, scaled_X_test, y_test, target_names, 34, \"Random Forest\")\npickle_save(rfc, \"rfc_34\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:14:19.623924Z","iopub.execute_input":"2024-12-06T19:14:19.624262Z","iopub.status.idle":"2024-12-06T19:14:45.548107Z","shell.execute_reply.started":"2024-12-06T19:14:19.624234Z","shell.execute_reply":"2024-12-06T19:14:45.546674Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# exporting and displaying the class 34 metrics\nmetrics_34.to_csv('metrics_34.csv', index=True)\nmetrics_34","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:14:45.549893Z","iopub.execute_input":"2024-12-06T19:14:45.550374Z","iopub.status.idle":"2024-12-06T19:14:45.564796Z","shell.execute_reply.started":"2024-12-06T19:14:45.550327Z","shell.execute_reply":"2024-12-06T19:14:45.563556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of model names\nmodel_names = [\"Logistic Regression\", \"KNN\", \"Random Forest\"]\n\n# List of dataframes\ndataframes = [metrics_2, metrics_8, metrics_34]\n\n# List of classes\nclasses = [2, 8, 34]\n\n# List of metric names\nmetrics = [\"Accuracy\", \"Recall\", \"Precision\", \"F1-Score\"]\n\n# Create a color mapping dictionary for each model\nmodel_colors = {\n    \"Logistic Regression\": 'tab:blue',\n    \"KNN\": 'tab:red',\n    \"Random Forest\": 'tab:olive',\n}\n\n# Creating subplots for each metric\nfor metric in metrics:\n    fig, axs = plt.subplots(1, len(classes), figsize=(12, 4), sharey=True)\n    fig.suptitle(f\"{metric} Scores\")\n\n    for i, df in enumerate(dataframes):\n        axs[i].set_title(f\"Class {classes[i]}\")\n        \n        # Plotting the respective metric score for each model with specified color\n        for model_name in model_names:\n            color = model_colors[model_name]\n            axs[i].bar(model_name, df.loc[metric, model_name], color=color)\n        \n        # Set x-axis ticks and labels, rotating labels by 90 degrees\n        axs[i].set_xticks(range(len(model_names)))\n        axs[i].set_xticklabels(model_names, rotation=30)\n    \n    # Save the figure\n    plt.savefig(f'{metric}_scores.png', bbox_inches='tight')\n    \n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:14:45.566618Z","iopub.execute_input":"2024-12-06T19:14:45.567079Z","iopub.status.idle":"2024-12-06T19:14:49.121065Z","shell.execute_reply.started":"2024-12-06T19:14:45.567037Z","shell.execute_reply":"2024-12-06T19:14:49.119807Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Efficient Intrution Detection\nIn real-world application, efficiency is important","metadata":{}},{"cell_type":"markdown","source":"## Evaluation of Models Trained on Smaller Data Subsets Applied to Larger Datasets","metadata":{}},{"cell_type":"code","source":"# test model on larger percentage\nlabel8 = df8_s[\"label\"]\ndf8 = df8_s.drop([\"label\"],axis=1)\n\n# Convert NumPy array to DataFrame\ndf_processed = pd.DataFrame(pipeline.fit_transform(df8))\ndf_processed['label'] = label8\nscaled_X_train, scaled_X_test, y_train, y_test = split(df_processed)\ntarget_names = [\"Benign\", \"BruteForce\", \"DDoS\", \"Dos\", \"Mirai\", \"Recon\", \"Spoofing\", \"Web\"]\n\nrfc8_load = pickle_load(\"rfc_8\")\nevaluate(rfc8_load, scaled_X_test , y_test, target_names, 8, \"Random Forest\")\n\nknn8_load = pickle_load(\"knn_8\")\nevaluate(knn8_load, scaled_X_test , y_test, target_names, 8, \"KNN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:14:49.122791Z","iopub.execute_input":"2024-12-06T19:14:49.123261Z","iopub.status.idle":"2024-12-06T19:15:10.957667Z","shell.execute_reply.started":"2024-12-06T19:14:49.123221Z","shell.execute_reply":"2024-12-06T19:15:10.956313Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# extract the label value\nlabel8 = df8_m[\"label\"]\ndf8 = df8_m.drop([\"label\"],axis=1)\n\n# Convert NumPy array to DataFrame\ndf_processed = pd.DataFrame(pipeline.fit_transform(df8))\ndf_processed['label'] = label8\nscaled_X_train, scaled_X_test, y_train, y_test = split(df_processed)\ntarget_names = [\"Benign\", \"BruteForce\", \"DDoS\", \"Dos\", \"Mirai\", \"Recon\", \"Spoofing\", \"Web\"]\n\nrfc8_load = pickle_load(\"rfc_8\")\nevaluate(rfc8_load, scaled_X_test , y_test, target_names, 8, \"Random Forest\")\n\nknn8_load = pickle_load(\"knn_8\")\nevaluate(knn8_load, scaled_X_test , y_test, target_names, 8, \"KNN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:39:00.272155Z","iopub.execute_input":"2024-12-06T19:39:00.272628Z","iopub.status.idle":"2024-12-06T19:39:44.187985Z","shell.execute_reply.started":"2024-12-06T19:39:00.272595Z","shell.execute_reply":"2024-12-06T19:39:44.186665Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Autoencoder\n\nSee: \"Quantized Autoencoders (QAE)/Final Versions/QAE.py\"","metadata":{}},{"cell_type":"code","source":"!pip install tensorflow_model_optimization\n\nimport optuna\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import layers, regularizers, Sequential\nfrom tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Sequential\nimport tensorflow_model_optimization as tfmot\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport keras\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport os\n\n# for lib in [pd, sk, sns, sv, optuna, tf, keras]:\nfor lib in [pd, sk, sns]:\n    print(f\"Using {lib.__name__} {lib.__version__}\")\n\nRANDOM_SEED = 5\nPATH_TO_QAE_CLASS = \"../../Quantized Autoencoders (QAE)/Final Versions\"\ntf.random.set_seed(RANDOM_SEED)\n\"\"\"\n# Import the QAE class\nsys.path.append(os.path.abspath(PATH_TO_QAE_CLASS))\nfrom QAE import QAE\nprint(\"INFO: QAE class successfully imported.\")\n\"\"\"\n\nclass QAE:\n    \"\"\"\n    Creates a Quantized Autoencoder (QAE) for anomaly detection from a dataset composed of X_preprocessed and y.\n    Note that the \"y\" series has no importance on its own: we only use it to make the train-test split,\n    but in principle all your values in y should have the same label as you want anomaly detection,\n    not classification.\n\n    Last modification on the 8th of August 2024 by Alexandre Le Mercier.\n    \"\"\"\n    def __init__(self, X_preprocessed, y, random_seed=5, test_size:float=0.3, first_layer:int=128,\n                 verbose:bool=True, activation:str='relu', loss:str='mean_squared_error', optimizer:str='adam',\n                 epochs:int=50, batch_size:int=32, shuffle:bool=True, figures_path:str=\"\", name:str=\"autoencoder\",\n                 pruning_initial_sparsity:float=0.2, pruning_final_sparsity:float=0.8, pruning_begin_step:int=0,\n                 pruning_end_step_divider:int=32*10, pruning_epochs:int=10, pruning_batch_size:int=32,\n                 number_of_clusters:int=8, models_path:str=\"\", cluster_epochs:int=10, cluster_batch_size:int=32,\n                 use_regularization:bool=True, regularization_weights:float=0.001,threshold:float=None):\n\n        self.F1_score = None\n        self.recall = None\n        self.precision = None\n        self.accuracy = None\n        self.confusion_matrix = None\n        self.model_for_export = None\n        self.history = None\n        self.y_val = None\n        self.y_train = None\n        self.X_val = None\n        self.X_train = None\n        self.input_dim = None\n        self.autoencoder = None\n        self.pruning_end_step = None\n\n        self.X_preprocessed = X_preprocessed\n        self.y = y\n        self.seed = random_seed\n        self.test_size = test_size\n        self.first_layer = first_layer\n        self.verbose = verbose\n        self.activation = activation\n        self.loss = loss\n        self.optimizer = optimizer\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        self.figures_path = figures_path\n        self.name = name\n        self.pruning_initial_sparsity = pruning_initial_sparsity\n        self.pruning_final_sparsity = pruning_final_sparsity\n        self.pruning_begin_step = pruning_begin_step\n        self.pruning_end_step_divider = pruning_end_step_divider\n        self.pruning_epochs = pruning_epochs\n        self.pruning_batch_size = pruning_batch_size\n        self.number_of_clusters = number_of_clusters\n        self.models_path = models_path\n        self.cluster_epochs = cluster_epochs\n        self.cluster_batch_size = cluster_batch_size\n        self.use_regularization = use_regularization\n        self.regularization_weights = regularization_weights\n        self.threshold = threshold\n\n        tf.random.set_seed(self.seed)\n        np.random.seed(self.seed)\n\n        #assert tf.__version__ == \"2.13.0\", 'TensorFlow 2.13 required for compatibility with tfmot 0.8.0.'\n        assert self.first_layer % 8 == 0, 'The specified layer number is not dividable by 8.'\n\n        self.verbose_message(f\"QAE INFO: Bottleneck layer will be made of {self.first_layer//8} neurons.\")\n\n        self.split_data()\n        self.create_autoencoder_model()\n        self.compile_autoencoder()\n\n        self.verbose_message(\"QAE model successfully created. Execute self.train_autoencoder() when ready.\")\n\n    def split_data(self):\n        self.X_train, self.X_val, _, _ = train_test_split(self.X_preprocessed,\n                                                          self.y, test_size=self.test_size,\n                                                          random_state=self.seed)\n        self.input_dim = self.X_train.shape[1]\n        self.pruning_end_step =  len(self.X_train) // self.pruning_end_step_divider\n\n    def create_autoencoder_model(self):\n        # Function to optionally add regularizer\n        def add_dense_layer(units):\n            return layers.Dense(units, activation=self.activation,\n                                kernel_regularizer=regularizers.l2(self.regularization_weights) if self.use_regularization else None)\n\n        layers_list = [\n            add_dense_layer(self.first_layer),\n            add_dense_layer(self.first_layer // 2),\n            add_dense_layer(self.first_layer // 4),\n            add_dense_layer(self.first_layer // 8),\n            add_dense_layer(self.first_layer // 4),\n            add_dense_layer(self.first_layer // 2),\n            add_dense_layer(self.first_layer),\n            layers.Dense(self.input_dim, activation='sigmoid')\n        ]\n\n        self.autoencoder = Sequential(layers_list)\n        self.autoencoder.build(input_shape=(None, self.input_dim))\n\n    def compile_autoencoder(self):\n        self.autoencoder.compile(optimizer=self.optimizer, loss=self.loss)\n        if self.verbose:\n            self.autoencoder.summary()\n\n    def train_autoencoder(self):\n        self.history = self.autoencoder.fit(self.X_train, self.X_train,\n                                            epochs=self.epochs,\n                                            batch_size=self.batch_size,\n                                            shuffle=self.shuffle,\n                                            verbose=self.verbose,\n                                            validation_data=(self.X_val, self.X_val))\n        if self.verbose:\n            self.plot_learning_curves()\n        self.save_model(self.autoencoder, f\"{self.name}_model\")\n        self.model_for_export = self.autoencoder\n        self.verbose_message(\"QAE model successfully trained. Execute self.quantize_autoencoder()\"\n                             \" or self.prune_and_cluster_autoencoder() when ready.\")\n\n    def plot_learning_curves(self):\n        plt.figure(figsize=(10, 6))\n        plt.plot(self.history.history['loss'], label='Training Loss')\n        plt.plot(self.history.history['val_loss'], label='Validation Loss')\n        plt.title('Learning Curves')\n        plt.xlabel('Epochs')\n        plt.ylabel('Loss')\n        plt.legend()\n        plt.savefig(self.figures_path + \"QAE_learning_curves.png\")\n        plt.show()\n\n\n    def representative_data_gen(self):\n        for input_value in tf.data.Dataset.from_tensor_slices(self.X_train).batch(1).take(100):\n            yield [tf.cast(input_value, tf.float32)]\n\n    def quantize_parameters_and_activation_functions(self, model, name=None):\n        if name is None:\n            name = f\"{self.name}_int_quant_model\"\n        converter = tf.lite.TFLiteConverter.from_keras_model(model)\n        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n        converter.representative_dataset = self.representative_data_gen\n        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n        converter.inference_input_type = tf.uint8\n        converter.inference_output_type = tf.uint8\n        tflite_int_quant_model = converter.convert()\n        self.save_model(tflite_int_quant_model, name, '.tflite')\n\n    def quantize_autoencoder(self):\n        converter = tf.lite.TFLiteConverter.from_keras_model(self.autoencoder)\n        tflite_model = converter.convert()\n        self.save_model(tflite_model, f'{self.name}_model', '.tflite')\n        self.quantize_parameters_and_activation_functions(self.autoencoder)\n\n    def prune_and_cluster_autoencoder(self):\n        self.prune_autoencoder()\n        self.cluster_autoencoder()\n\n    def prune_autoencoder(self, quant=True):\n        pruning_params = {\n            'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(\n                initial_sparsity=self.pruning_initial_sparsity,\n                final_sparsity=self.pruning_final_sparsity,\n                begin_step=self.pruning_begin_step,\n                end_step=self.pruning_end_step\n            )\n        }\n        config = self.autoencoder.get_config()\n        weights = self.autoencoder.get_weights()\n        self.autoencoder = keras.models.Sequential.from_config(config)\n        self.autoencoder.set_weights(weights)\n        pruned_model = tfmot.sparsity.keras.prune_low_magnitude(self.autoencoder, **pruning_params)\n        pruned_model.compile(optimizer=self.optimizer, loss=self.loss)\n        callbacks = [tfmot.sparsity.keras.UpdatePruningStep()]\n        pruned_model.fit(self.X_train, self.X_train,\n                         epochs=self.pruning_epochs,\n                         batch_size=self.pruning_batch_size,\n                         validation_data=(self.X_val, self.X_val),\n                         verbose=self.verbose,\n                         callbacks=callbacks)\n        self.model_for_export = tfmot.sparsity.keras.strip_pruning(pruned_model)\n        self.save_model(self.model_for_export, f'pruned_{self.name}_model')\n        if quant is True:\n            self.quantize_parameters_and_activation_functions(self.model_for_export, f\"{self.name}_pruned_model\")\n        else:\n            self.save_model(self.model_for_export, f\"{self.name}_pruned_model\", '.keras')\n\n    def cluster_autoencoder(self, quant=True, use_pruned_model=False):\n        clustering_params = {\n            'number_of_clusters': self.number_of_clusters,\n            'cluster_centroids_init': tfmot.clustering.keras.CentroidInitialization.KMEANS_PLUS_PLUS\n        }\n        if use_pruned_model:\n            clustered_model = tfmot.clustering.keras.cluster_weights(self.model_for_export, **clustering_params)\n        else:\n            clustered_model = tfmot.clustering.keras.cluster_weights(self.autoencoder, **clustering_params)\n        clustered_model.compile(optimizer='adam', loss='mean_squared_error')\n        clustered_model.fit(self.X_train, self.X_train,\n                            epochs=self.cluster_epochs,\n                            batch_size=self.cluster_batch_size,\n                            validation_data=(self.X_val, self.X_val))\n        final_clustered_model = tfmot.clustering.keras.strip_clustering(clustered_model)\n        self.save_model(final_clustered_model, f'clustered_{self.name}_model')\n        if quant:\n            self.quantize_parameters_and_activation_functions(final_clustered_model, f'{self.name}_clustered_model')\n        else:\n            self.save_model(final_clustered_model, f\"{self.name}_clustered_model\", '.keras')\n\n    def test_on_validation_set(self, path_to_model, X_benign_val_preprocessed=None, X_anomaly_preprocessed=None,\n                               percentage_false_negatives: float = 0.05, automatic_threshold: bool = False, silence=False,\n                               manual_threshold=None, anomaly_if=\"higher\"):\n        # Load the model\n        autoencoder = tf.keras.models.load_model(path_to_model)\n\n        # Plotting function remains unchanged\n        def plot_reconstruction_error_boxplot(reconstruction_error, threshold, title):\n            plt.figure(figsize=(10, 6))\n            sns.boxplot(x=reconstruction_error, color='blue')\n            plt.axvline(threshold, color='red', linestyle='--', linewidth=2, label=f'Threshold: {threshold}')\n            plt.title(title)\n            plt.xlabel('Reconstruction Error')\n            plt.legend()\n            plt.xlim([0, np.percentile(reconstruction_error, 99)])\n            plt.show()\n\n        # Calculate the threshold based on the training data\n        X_benign_train_reconstructed = autoencoder.predict(self.X_preprocessed)\n        reconstruction_error_train = tf.keras.losses.mean_squared_error(self.X_preprocessed, X_benign_train_reconstructed).numpy()\n\n        if self.threshold is not None:\n            threshold = self.threshold\n        elif automatic_threshold:\n            threshold = np.mean(reconstruction_error_train) + np.std(reconstruction_error_train)\n        elif manual_threshold is not None:\n            threshold = manual_threshold\n        else:\n            threshold = np.percentile(reconstruction_error_train, 100 * (1.0 - percentage_false_negatives))\n\n        self.threshold = threshold\n\n        # Plot boxplot for training data\n        if not silence:\n            plot_reconstruction_error_boxplot(reconstruction_error_train, threshold, title='RE Boxplot on Training Data')\n\n        if X_anomaly_preprocessed is None and X_benign_val_preprocessed is None:\n            sets = []\n        elif X_anomaly_preprocessed is None:\n            sets = [(X_benign_val_preprocessed, 'Validation Benign Data')]\n        else:\n            sets = [(X_anomaly_preprocessed, 'Malicious Data'), (X_benign_val_preprocessed, 'Validation Benign Data')]\n\n        # Initialize confusion matrix components\n        TP = FP = TN = FN = 0\n\n        for X, description in sets:\n            X_reconstructed = autoencoder.predict(X)\n            reconstruction_error = tf.keras.losses.mean_squared_error(X, X_reconstructed).numpy()\n\n            # Plot boxplot for validation sets\n            if not silence:\n                plot_reconstruction_error_boxplot(reconstruction_error, threshold, title=f'RE Boxplot on {description}')\n\n            # Check for anomalies based on reconstruction error\n            if anomaly_if == \"higher\":\n                anomalies = reconstruction_error > threshold\n            elif anomaly_if == \"lower\":\n                anomalies = reconstruction_error < threshold\n            else:\n                print(\"'anomaly_if'  be either 'higher' or 'lower'.\")\n                return\n\n            # Calculate TP, FP, TN, FN based on description\n            if description == 'Validation Benign Data':\n                TN += np.sum(~anomalies)  # True Negatives: Benign correctly identified\n                FP += np.sum(anomalies)   # False Positives: Benign incorrectly identified as anomaly\n            elif description == 'Malicious Data':\n                TP += np.sum(anomalies)   # True Positives: Malicious correctly identified as anomaly\n                FN += np.sum(~anomalies)  # False Negatives: Malicious incorrectly identified as benign\n\n        if X_anomaly_preprocessed is not None and X_benign_val_preprocessed is not None:\n            self.accuracy = (TP + TN) / (TP + FP + TN + FN)\n            self.precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n            self.recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n            self.F1_score = 2 * (self.precision * self.recall) / (self.precision + self.recall) if (self.precision + self.recall) > 0 else 0\n            self.confusion_matrix = [\n                [TN, FP],\n                [FN, TP]\n            ]\n\n            if not silence:\n                print(\"Accuracy:\", self.accuracy)\n                print(\"Precision:\", self.precision)\n                print(\"Recall:\", self.recall)\n                print(\"F1 score:\", self.F1_score)\n                print(\"Confusion matrix:\", self.confusion_matrix)\n\n\n        # Plotting distribution of reconstruction errors\n        \"\"\"\n        if not silence:\n            plt.figure(figsize=(12,8))\n            sns.set(font_scale=2)\n            sns.set_style(\"white\")\n            sns.histplot(reconstruction_error_train, bins=50, kde=True, color='grey', linewidth=3, label='Train Loss')\n            plt.axvline(x=np.mean(reconstruction_error_train), color='g', linestyle='--', linewidth=3)\n            plt.text(np.mean(reconstruction_error_train), 200, \"Normal Mean\", horizontalalignment='center', \n                     size='small', color='black', weight='semibold')\n            plt.axvline(x=threshold, color='b', linestyle='--', linewidth=3)\n            plt.text(threshold, 250, \"Threshold\", horizontalalignment='center', \n                     size='small', color='blue', weight='semibold')\n\n            for i, loss in enumerate(test_loss):\n                color = 'red' if i == 0 else 'blue'\n                label = 'Test Loss' if i == 0 else 'Validation Loss'\n                sns.histplot(loss, bins=50, kde=True, color=color, linewidth=3, label=label)\n                plt.axvline(x=np.mean(loss), color='g', linestyle='--', linewidth=3)\n                plt.text(np.mean(loss), 200, f\"{label} Mean\", horizontalalignment='center', \n                         size='small', color='black', weight='semibold')\n                plt.axvline(x=threshold, color='b', linestyle='--', linewidth=3)\n\n            plt.xlabel(\"Loss\")\n            plt.ylabel(\"Number of Examples\")\n            plt.legend()\n            sns.despine()\n            plt.show()\n        \"\"\"\n\n    def verbose_message(self, message):\n        if self.verbose:\n            print(message)\n\n    def save_model(self, model, name, ext='.keras'):\n        if ext == \".keras\" or ext == \".h5\":\n            model.save(self.models_path + name + ext)\n        elif ext == \".tflite\":\n            with open(self.models_path + name + ext, 'wb') as f:\n                f.write(model)\n        self.verbose_message(f\"Model {self.models_path + name + ext} saved. \"\n                             f\"Size: {os.path.getsize(self.models_path + name + ext) / 1024} KB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:15:33.019543Z","iopub.execute_input":"2024-12-06T19:15:33.019934Z","iopub.status.idle":"2024-12-06T19:15:43.624198Z","shell.execute_reply.started":"2024-12-06T19:15:33.019894Z","shell.execute_reply":"2024-12-06T19:15:43.622250Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Choosing the threshold visually\nbest_params = {\n    'name': \"best_optuna_ben050.0_mal000.1_qae\",\n    'first_layer': 288, 'activation': 'tanh', 'optimizer': 'adam', 'batch_size': 112, 'regularization_weights': 0.004035483817578062,\n    'epochs': 20,\n    'use_regularization': True\n}\nparams = {\n        'name': \"Models/final_opt_qae\",\n        'first_layer': best_params['first_layer'],\n        'activation': best_params['activation'],\n        'optimizer': best_params['optimizer'],\n        'batch_size': best_params['batch_size'],\n        'regularization_weights': best_params['regularization_weights'],\n        'epochs': 20,\n        'use_regularization': True\n}\n\n#lim = 1000000\n# qae = QAE(benign_df_tr, pd.Series([\"is_benign\"]*benign_df_tr.shape[0]), **params)\n# qae.threshold=150\n# qae.train_autoencoder()","metadata":{"ExecuteTime":{"end_time":"2024-08-21T12:38:00.047779Z","start_time":"2024-08-21T12:34:05.244498Z"},"trusted":true,"execution":{"iopub.status.busy":"2024-12-06T19:15:43.626214Z","iopub.execute_input":"2024-12-06T19:15:43.626593Z","iopub.status.idle":"2024-12-06T19:15:43.634063Z","shell.execute_reply.started":"2024-12-06T19:15:43.626554Z","shell.execute_reply":"2024-12-06T19:15:43.632901Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Potential integration with IPS(Intrusion Prevention System)\nFeature Importance Study ","metadata":{}}]}